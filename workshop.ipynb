{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4b9724-7cd4-4055-9eeb-d91511eb0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33996a9c-2f7a-4256-89bc-499a855aa784",
   "metadata": {},
   "source": [
    "# Automatic gradient functionality\n",
    "\n",
    "When you design a neural network, we humans think in a forward manner: we want data to get into the network and pass through layers and we usually can visualise what transformations of the data can or should be applied so we achieve our goal. Thus, the design and then the implementation of the so called forward pass through a network is something we humans can naturally grasp in our minds. \n",
    "\n",
    "But the way that neural networks are trained requires that all transformations of the forward pass, no matter how complex, are differentiated with respect to the weights of the neural network and all those gradients play then a role in weight update. The more complex were the transformations of the forward pass, the more complex will be the calculation of corresponding gradient functions. It is very inflexible, error prone and in some cases simply imposible or very hard to execute. It is a task for a machine.\n",
    "\n",
    "The appearance of the dynamic expression tree building with automatic gradient calculation ability let us, humans, design only the forward pass (i.e. how the data is transformed in the neural network), while the engine of the framework takes care of correctly executing the backward pass. It gave the design of neural networks unseen flexibility and gave the whole Machine Learning field the boost we are experiencing nowadays.\n",
    "\n",
    "One of the examples of frameworks that support dynamic expression tree building with automatic gradient calculation ability is PyTorch.\n",
    "\n",
    "So let's see how it works in practice in Pytorch. We don't need a neural network to see it.\n",
    "\n",
    "Let's have $f=x^3+y^2$ as an example. \n",
    "Then you can calculate by hand: \n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} = 3x^2, \\frac{\\delta f}{\\delta y} = 2y$\n",
    "\n",
    "Concretely, as an example with which we will work some more:\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} |_{x=2} = 12$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} |_{x=3} = 27$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y} |_{y=4} = 8$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y} |_{y=5} = 10$\n",
    "\n",
    "You get all this automatically with PyTorch, which builds a tree of an expression that is constructed as we go. Let's have an example in Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798a24e2-370b-4a02-b770-0c580b1e9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([4.], requires_grad=True)\n",
    "f = x**3 + y**2\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70eb4a9-169c-4b16-8ea5-b63d66566e93",
   "metadata": {},
   "source": [
    "To calculate gradients you need to call `backward()`. Let's see if we get\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} |_{x=2} = 12$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y} |_{y=4} = 8$\n",
    "\n",
    "as expected. The gradients of $f$ with respect to variables $x$ and $y$ are held in the `grad` field in those variables (`x.grad`, `y.grad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cd43a2-8cd2-462d-9ed4-30b960d15cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd52f90-4024-4185-b188-afcbd39cb995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfe398-3271-4a6f-ae21-ba84bdd49e55",
   "metadata": {},
   "source": [
    "OK, so far so good. But the weights in a neural network are not scalars, they are multidimensional entities, most commonly they are two dimensional matrices. In PyTorch, two dimensional matrices are called tensors of order two. What if $x$ and $y$ were tensors of order two? Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a3db72-56ef-4abe-87d9-e1143ea92488",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2., 3., 2.], [2., 3., 3.]], requires_grad=True)\n",
    "y = torch.tensor([[4., 5., 5.], [4., 5., 5.]], requires_grad=True)\n",
    "f = x**3 + y**2\n",
    "f.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ec77c-ec04-4bcf-aafa-fe6f959ef21b",
   "metadata": {},
   "source": [
    "Notice the last line, `f.sum().backward()`. Recall, that to calculate gradients you need to call `backward()`, but you may call it on scalar variables only, because loss that we calculate for a neural network is a scalar. Hence, we use such a trick to force PyTorch to calculate our gradients for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606424a1-df7e-49bb-8d7d-b6911f8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 27., 12.],\n",
       "        [12., 27., 27.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010a799c-9fe4-43c2-bba8-a03eb06e1b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8., 10., 10.],\n",
       "        [ 8., 10., 10.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd09e7-a78f-406f-bb78-8f67c6744ea6",
   "metadata": {},
   "source": [
    "OK, results are as expected (i.e. as calculated by hand earlier)\n",
    "\n",
    "Now, let us consider another function. \n",
    "\n",
    "$f=xy$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} = y$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y} = x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b542296-48a4-44fb-b06d-d7a7c5ef2338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9.],\n",
       "        [-1.,  1.,  2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3.], [2., 4., 6.]], requires_grad=True)\n",
    "y = torch.tensor([[3., 6., 9.], [-1., 1., 2.]], requires_grad=True)\n",
    "f = x*y\n",
    "f.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a37af2-c206-4d25-a596-3304c75d8b06",
   "metadata": {},
   "source": [
    "Well, $\\frac{\\delta f}{\\delta x} = y$ doesn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eaf4d-99e2-4f20-8779-6f3e6bd23e45",
   "metadata": {},
   "source": [
    "# Your task\n",
    "\n",
    "Your task is to calculate values of\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} |_{x=2.0, y=3.0}$\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y} |_{x=1.5, y=-1.5}$\n",
    "\n",
    "for\n",
    "\n",
    "$f=\\frac{sin(xy)}{sinx}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683df9b-d200-4606-8482-9104d65878b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
