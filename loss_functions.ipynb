{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1389d586-2bf4-4817-924d-b5082a7d4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bee46-53a6-455b-85e5-258f6810a41a",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "In neural network supervised training (and to lesser extent in evauation) one of the key concepts is the loss function. The loss function measures what is the distance between the known target values and neural network predictions. A good loss function is differentiable and has non-zero gradients. \n",
    "\n",
    "As an example, Accuracy, a measure often used in neural network evaluation in classification tasks, has gradients almost everywhere equal to zero. This is not good to train the neural network, which needs to update the weights in a process called backpropagation, and for that it needs non-zero gradients. Thus, Accuracy isn't suitable as a loss function.\n",
    "\n",
    "Examples of loss functions:\n",
    "\n",
    "- Mean Square Error - MSE - perfect for regression,\n",
    "- Hinge Loss - classification loss,\n",
    "- Binary Cross Entropy - a measure used for classification with two classes, it approximates Accuracy but has non-zero gradients,\n",
    "- Cross Entropy - a measure used for general classification. \n",
    "\n",
    "Let us provide an exact definition of MSE.\n",
    "\n",
    "If a target vector is\n",
    "\n",
    "$T=(T_i), i=1, \\ldots, N$\n",
    "\n",
    "and a prediction vector of a regressor is \n",
    "\n",
    "$P=(P_i), i=1, \\ldots, N$\n",
    "\n",
    "Then $MSE(P, T) = \\frac{\\sum_{i=1}^N (T_i-P_i)^2}{N}$\n",
    "\n",
    "## Your task\n",
    "\n",
    "Calculate, using Python, MSE loss of the prediction $P=(1.1, 4.12, 8.9, 14.85)$ versus the target values $T=(1,4,9,16)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d557ccbd-da3d-4184-a3dd-d2f45acedb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3392250000000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target = [1, 4, 9, 16]\n",
    "prediction = [1.1, 4.12, 8.9, 14.85]\n",
    "sum = 0.0\n",
    "for i, pred in enumerate(prediction):\n",
    "    sum += (target[i]-pred)**2\n",
    "\n",
    "print(sum/len(target))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d06270-eb68-428e-9661-202cafbe6fc0",
   "metadata": {},
   "source": [
    "PyTorch has predefined the loss functions. Of course, one is free to define his own loss functions, too, but a predefined loss functions have some advantages\n",
    "- the implementation is numerically stable. As an example, Binary Cross Entropy has a logarithm following the exponent. If you do that correctly, the result is identity. But the exponent of even moderately large values is infinite in numerical calculations.\n",
    "- they usually have more efficient implementations\n",
    "- they have built-in reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e85b939-3b1a-4029-acf9-3da47d473f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3392)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7844f0-1cbd-45ac-87b0-c757d9ff0959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3569)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c5c84d-1ec7-4964-b41b-4d29b7aadeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.0144, 0.0100, 1.3225])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42802f49-30ae-4532-ab0b-edf1b9cbd73d",
   "metadata": {},
   "source": [
    "We can see that the fourth element of a prediction error had a largest contribution in a MSE by using `reduction = \"none\"`. If you want to sum the values, rather than average them, use `reduction = \"sum\"`. \n",
    "\n",
    "## Classification loss\n",
    "\n",
    "### Two classes - Binary Cross Entropy\n",
    "\n",
    "OK, now let's examine other loss functions, the ones that are used for classification. If the target is only classes 0 or 1, and predictions are floats between 0 and 1, then it is binary classification and the appropriate loss is Binary Cross Entropy loss with the following usage example in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4926174c-31c5-4cfc-93e0-3af3b31e2d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2614, 0.1165, 0.0101])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy(torch.tensor([0.0, 0.77, 0.11, 0.99]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c9724-b096-45ee-b3f8-19cbaa69226f",
   "metadata": {},
   "source": [
    "If the target is only classes 0 or 1, and predictions are arbitrary floats, then it is still binary classification but before using Binary Cross Entropy loss the values should be transformed first into $<0,1>$ interval with the use of a Sigmoid: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d14579ae-7ea3-493c-b694-36fcb483d47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.0000e+02])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy(torch.sigmoid(torch.tensor([-2.0, 3.13, 0.0, -120.0])), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf37b51-fc2e-4b6b-9325-95a4b827cea0",
   "metadata": {},
   "source": [
    "Or, you can apply a sigmoid automatically (which is recommended because of numerical stability) with using `torch.nn.functional.binary_cross_entropy_with_logits()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c366a7-793e-4168-8fa2-2854b2a53184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.2000e+02])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-2.0, 3.13, 0.0, -120.0]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43243162-d179-4917-ae44-39dac88763dd",
   "metadata": {},
   "source": [
    "### Arbitrary number of classes - Cross Entropy\n",
    "\n",
    "It used for multiclass classification, but - in principle - there is nothing stopping us from using it for two classes, too. Please observe, that instead of a vector of logits, you must provide the loss function with raw predictions for all classes (thus we provide a tensor of increased order as predictions). A softmax is executed internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003f5c2f-68b6-42e0-8f66-1ab226473f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931, 0.6931, 0.6931, 0.9741])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(torch.sigmoid(torch.tensor([[-2.0, -2.0], [3.14, 3.14], [0.0, 0.0], [120.0, 0.0]])), torch.tensor([0,1,0,1]).type(torch.long), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91aa5a-7c5e-4bcd-b721-dd6e20a35005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b76c5-e1de-487d-bae2-e941ea66febf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
