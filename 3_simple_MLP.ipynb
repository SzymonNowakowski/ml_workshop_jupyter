{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/ml_workshop_jupyter/blob/main/3_simple_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers\n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*).\n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "### Answers to task #1\n",
        "A nonlinear component in-between the linear layers is essential:\n",
        "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network.\n",
        "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits from *zero* to *nine*. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "**You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).**\n",
        "\n",
        "# Mathematically oriented notation for the MLP\n",
        "\n",
        "A three layer perceptron we will work further with in this workshop can be defined in a mathematematically apealing way as\n",
        "\n",
        "$f:\\mathbb{R}^{28\\cdot 28 + D} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
        "\n",
        "$f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4$,\n",
        "\n",
        "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and $\\left[ \\cdot \\right]_+$ is taking a positive part, which is another notation for ReLU.\n",
        "\n",
        "Note, that there is no nonlinear activation after the third layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n",
        "\n",
        "# Automatic gradient\n",
        "\n",
        "The [automatic gradient functionality covered in another workshop](https://github.com/SzymonNowakowski/ml_workshop_jupyter/blob/main/1_computational_graph.ipynb) will be used to automatically calculate\n",
        "gradient of\n",
        "$loss \\left(f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right), y_i\\right)$ for the training set $\\left(x_i, y_i \\right)_{i=1, \\ldots, N}$\n",
        "with respect to each component of $W_i$ and $b_i$ tensors.\n",
        "\n",
        "The training set will be batched, but it is just a technical detail.\n",
        "\n",
        "# Loss\n",
        "\n",
        "For the loss function we will use a crossentropy loss directly from PyTorch functional library `torch.nn.functional`. You can read more about this loss [in PyTorch documentation about CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) or generally in [`torch.nn.functional` loss functions section](https://pytorch.org/docs/2.1/nn.functional.html#loss-functions).\n",
        "\n",
        "### Workshop dedicated to loss functions\n",
        "\n",
        "**Also, [there is a workshop dedicated to loss functions](https://github.com/SzymonNowakowski/ml_workshop_jupyter/blob/main/2_loss_functions.ipynb)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68",
      "metadata": {
        "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
        "outputId": "48795994-8198-4dc3-d84b-1ca2428445fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 179248146.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 37456924.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 41615617.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20957677.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "80535975-0e47-4f4c-c138-f612c25c01f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "54431961-3050-455c-a590-7afccefd5bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "bf797343-5427-435a-8073-da179d05e824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:*\n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
      "metadata": {
        "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
        "outputId": "7d78a14d-9975-454b-8c57-745f66c773a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1306604762738429, 0.30810780385646264)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "89bff416-f972-4a3e-8a52-beff156fbb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([1, 7, 9,  ..., 0, 7, 4])\n",
            "1 -th batch labels : tensor([0, 5, 4,  ..., 7, 4, 1])\n",
            "2 -th batch labels : tensor([7, 4, 9,  ..., 6, 7, 0])\n",
            "3 -th batch labels : tensor([9, 2, 4,  ..., 8, 0, 1])\n",
            "4 -th batch labels : tensor([1, 8, 5,  ..., 2, 3, 0])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch.\n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
      "metadata": {
        "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
        "outputId": "79b1de0f-12c8-49bb-ae6b-d454f3614300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
      "metadata": {
        "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168"
      },
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
        "                                          #that output from a preceding layer serves as input to the next layer\n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "9ff12833-120b-4ead-dd4b-f32b145cc321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.3068065643310547\n",
            "epoch: 0 batch: 1 current batch loss: 2.0909082889556885\n",
            "epoch: 0 batch: 2 current batch loss: 1.6373933553695679\n",
            "epoch: 0 batch: 3 current batch loss: 1.1119616031646729\n",
            "epoch: 0 batch: 4 current batch loss: 0.997447669506073\n",
            "epoch: 0 batch: 5 current batch loss: 1.2922513484954834\n",
            "epoch: 0 batch: 6 current batch loss: 1.3425639867782593\n",
            "epoch: 0 batch: 7 current batch loss: 1.1184653043746948\n",
            "epoch: 0 batch: 8 current batch loss: 0.6982258558273315\n",
            "epoch: 0 batch: 9 current batch loss: 0.704410970211029\n",
            "epoch: 0 batch: 10 current batch loss: 0.7593024373054504\n",
            "epoch: 0 batch: 11 current batch loss: 0.6717523336410522\n",
            "epoch: 0 batch: 12 current batch loss: 0.6541317701339722\n",
            "epoch: 0 batch: 13 current batch loss: 0.5578519105911255\n",
            "epoch: 0 batch: 14 current batch loss: 0.5410290360450745\n",
            "epoch: 0 batch: 15 current batch loss: 0.5218665599822998\n",
            "epoch: 0 batch: 16 current batch loss: 0.473332941532135\n",
            "epoch: 0 batch: 17 current batch loss: 0.4812173545360565\n",
            "epoch: 0 batch: 18 current batch loss: 0.4774669110774994\n",
            "epoch: 0 batch: 19 current batch loss: 0.40541571378707886\n",
            "epoch: 0 batch: 20 current batch loss: 0.3797890245914459\n",
            "epoch: 0 batch: 21 current batch loss: 0.3611779808998108\n",
            "epoch: 0 batch: 22 current batch loss: 0.3927077353000641\n",
            "epoch: 0 batch: 23 current batch loss: 0.3651565611362457\n",
            "epoch: 0 batch: 24 current batch loss: 0.3503597676753998\n",
            "epoch: 0 batch: 25 current batch loss: 0.3675200045108795\n",
            "epoch: 0 batch: 26 current batch loss: 0.3050827980041504\n",
            "epoch: 0 batch: 27 current batch loss: 0.30666470527648926\n",
            "epoch: 0 batch: 28 current batch loss: 0.28056490421295166\n",
            "epoch: 0 batch: 29 current batch loss: 0.3516475558280945\n",
            "epoch: 1 batch: 0 current batch loss: 0.2884334325790405\n",
            "epoch: 1 batch: 1 current batch loss: 0.2941915690898895\n",
            "epoch: 1 batch: 2 current batch loss: 0.291998952627182\n",
            "epoch: 1 batch: 3 current batch loss: 0.2963065505027771\n",
            "epoch: 1 batch: 4 current batch loss: 0.22850769758224487\n",
            "epoch: 1 batch: 5 current batch loss: 0.26518672704696655\n",
            "epoch: 1 batch: 6 current batch loss: 0.23629215359687805\n",
            "epoch: 1 batch: 7 current batch loss: 0.25251075625419617\n",
            "epoch: 1 batch: 8 current batch loss: 0.23398488759994507\n",
            "epoch: 1 batch: 9 current batch loss: 0.20979627966880798\n",
            "epoch: 1 batch: 10 current batch loss: 0.2270846664905548\n",
            "epoch: 1 batch: 11 current batch loss: 0.2015102505683899\n",
            "epoch: 1 batch: 12 current batch loss: 0.23277267813682556\n",
            "epoch: 1 batch: 13 current batch loss: 0.22653071582317352\n",
            "epoch: 1 batch: 14 current batch loss: 0.2012001872062683\n",
            "epoch: 1 batch: 15 current batch loss: 0.21916154026985168\n",
            "epoch: 1 batch: 16 current batch loss: 0.20510296523571014\n",
            "epoch: 1 batch: 17 current batch loss: 0.1885336935520172\n",
            "epoch: 1 batch: 18 current batch loss: 0.19686391949653625\n",
            "epoch: 1 batch: 19 current batch loss: 0.17125344276428223\n",
            "epoch: 1 batch: 20 current batch loss: 0.17583449184894562\n",
            "epoch: 1 batch: 21 current batch loss: 0.17396484315395355\n",
            "epoch: 1 batch: 22 current batch loss: 0.17518025636672974\n",
            "epoch: 1 batch: 23 current batch loss: 0.2001398503780365\n",
            "epoch: 1 batch: 24 current batch loss: 0.1680631935596466\n",
            "epoch: 1 batch: 25 current batch loss: 0.18759366869926453\n",
            "epoch: 1 batch: 26 current batch loss: 0.1685595065355301\n",
            "epoch: 1 batch: 27 current batch loss: 0.21127630770206451\n",
            "epoch: 1 batch: 28 current batch loss: 0.17512355744838715\n",
            "epoch: 1 batch: 29 current batch loss: 0.15919464826583862\n",
            "epoch: 2 batch: 0 current batch loss: 0.1608765721321106\n",
            "epoch: 2 batch: 1 current batch loss: 0.15914404392242432\n",
            "epoch: 2 batch: 2 current batch loss: 0.17191050946712494\n",
            "epoch: 2 batch: 3 current batch loss: 0.1383620798587799\n",
            "epoch: 2 batch: 4 current batch loss: 0.14839720726013184\n",
            "epoch: 2 batch: 5 current batch loss: 0.14276479184627533\n",
            "epoch: 2 batch: 6 current batch loss: 0.13492780923843384\n",
            "epoch: 2 batch: 7 current batch loss: 0.12912894785404205\n",
            "epoch: 2 batch: 8 current batch loss: 0.1429782211780548\n",
            "epoch: 2 batch: 9 current batch loss: 0.1332383006811142\n",
            "epoch: 2 batch: 10 current batch loss: 0.1454339474439621\n",
            "epoch: 2 batch: 11 current batch loss: 0.14257153868675232\n",
            "epoch: 2 batch: 12 current batch loss: 0.14318983256816864\n",
            "epoch: 2 batch: 13 current batch loss: 0.10989689826965332\n",
            "epoch: 2 batch: 14 current batch loss: 0.14607951045036316\n",
            "epoch: 2 batch: 15 current batch loss: 0.12523449957370758\n",
            "epoch: 2 batch: 16 current batch loss: 0.14857763051986694\n",
            "epoch: 2 batch: 17 current batch loss: 0.13745298981666565\n",
            "epoch: 2 batch: 18 current batch loss: 0.13130460679531097\n",
            "epoch: 2 batch: 19 current batch loss: 0.11498090624809265\n",
            "epoch: 2 batch: 20 current batch loss: 0.12353796511888504\n",
            "epoch: 2 batch: 21 current batch loss: 0.11226631700992584\n",
            "epoch: 2 batch: 22 current batch loss: 0.135706827044487\n",
            "epoch: 2 batch: 23 current batch loss: 0.12966321408748627\n",
            "epoch: 2 batch: 24 current batch loss: 0.11227907985448837\n",
            "epoch: 2 batch: 25 current batch loss: 0.13156527280807495\n",
            "epoch: 2 batch: 26 current batch loss: 0.11297835409641266\n",
            "epoch: 2 batch: 27 current batch loss: 0.12155459821224213\n",
            "epoch: 2 batch: 28 current batch loss: 0.1287386566400528\n",
            "epoch: 2 batch: 29 current batch loss: 0.11880175769329071\n",
            "epoch: 3 batch: 0 current batch loss: 0.12517854571342468\n",
            "epoch: 3 batch: 1 current batch loss: 0.10907381027936935\n",
            "epoch: 3 batch: 2 current batch loss: 0.1149883046746254\n",
            "epoch: 3 batch: 3 current batch loss: 0.08933009952306747\n",
            "epoch: 3 batch: 4 current batch loss: 0.09714358299970627\n",
            "epoch: 3 batch: 5 current batch loss: 0.11771222203969955\n",
            "epoch: 3 batch: 6 current batch loss: 0.10655777901411057\n",
            "epoch: 3 batch: 7 current batch loss: 0.08963752537965775\n",
            "epoch: 3 batch: 8 current batch loss: 0.11045994609594345\n",
            "epoch: 3 batch: 9 current batch loss: 0.09110336750745773\n",
            "epoch: 3 batch: 10 current batch loss: 0.10070988535881042\n",
            "epoch: 3 batch: 11 current batch loss: 0.09107022732496262\n",
            "epoch: 3 batch: 12 current batch loss: 0.11760435998439789\n",
            "epoch: 3 batch: 13 current batch loss: 0.08500762283802032\n",
            "epoch: 3 batch: 14 current batch loss: 0.08173107355833054\n",
            "epoch: 3 batch: 15 current batch loss: 0.0886986181139946\n",
            "epoch: 3 batch: 16 current batch loss: 0.09466826915740967\n",
            "epoch: 3 batch: 17 current batch loss: 0.09555406868457794\n",
            "epoch: 3 batch: 18 current batch loss: 0.09209762513637543\n",
            "epoch: 3 batch: 19 current batch loss: 0.06626639515161514\n",
            "epoch: 3 batch: 20 current batch loss: 0.11165861785411835\n",
            "epoch: 3 batch: 21 current batch loss: 0.10978414118289948\n",
            "epoch: 3 batch: 22 current batch loss: 0.10651393234729767\n",
            "epoch: 3 batch: 23 current batch loss: 0.08809742331504822\n",
            "epoch: 3 batch: 24 current batch loss: 0.08383163064718246\n",
            "epoch: 3 batch: 25 current batch loss: 0.09053637832403183\n",
            "epoch: 3 batch: 26 current batch loss: 0.09249914437532425\n",
            "epoch: 3 batch: 27 current batch loss: 0.07741906493902206\n",
            "epoch: 3 batch: 28 current batch loss: 0.09192994982004166\n",
            "epoch: 3 batch: 29 current batch loss: 0.09028880298137665\n",
            "epoch: 4 batch: 0 current batch loss: 0.09078594297170639\n",
            "epoch: 4 batch: 1 current batch loss: 0.07508103549480438\n",
            "epoch: 4 batch: 2 current batch loss: 0.0866858959197998\n",
            "epoch: 4 batch: 3 current batch loss: 0.06282557547092438\n",
            "epoch: 4 batch: 4 current batch loss: 0.08911975473165512\n",
            "epoch: 4 batch: 5 current batch loss: 0.08077988028526306\n",
            "epoch: 4 batch: 6 current batch loss: 0.060681041330099106\n",
            "epoch: 4 batch: 7 current batch loss: 0.08126545697450638\n",
            "epoch: 4 batch: 8 current batch loss: 0.07939160615205765\n",
            "epoch: 4 batch: 9 current batch loss: 0.07714756578207016\n",
            "epoch: 4 batch: 10 current batch loss: 0.06951712816953659\n",
            "epoch: 4 batch: 11 current batch loss: 0.053105831146240234\n",
            "epoch: 4 batch: 12 current batch loss: 0.06076440587639809\n",
            "epoch: 4 batch: 13 current batch loss: 0.06928721070289612\n",
            "epoch: 4 batch: 14 current batch loss: 0.06599467992782593\n",
            "epoch: 4 batch: 15 current batch loss: 0.08583231270313263\n",
            "epoch: 4 batch: 16 current batch loss: 0.06402067840099335\n",
            "epoch: 4 batch: 17 current batch loss: 0.0797196701169014\n",
            "epoch: 4 batch: 18 current batch loss: 0.06287381798028946\n",
            "epoch: 4 batch: 19 current batch loss: 0.07388566434383392\n",
            "epoch: 4 batch: 20 current batch loss: 0.08562734723091125\n",
            "epoch: 4 batch: 21 current batch loss: 0.09703166037797928\n",
            "epoch: 4 batch: 22 current batch loss: 0.06839215755462646\n",
            "epoch: 4 batch: 23 current batch loss: 0.07782653719186783\n",
            "epoch: 4 batch: 24 current batch loss: 0.07465676218271255\n",
            "epoch: 4 batch: 25 current batch loss: 0.06973801553249359\n",
            "epoch: 4 batch: 26 current batch loss: 0.07281642407178879\n",
            "epoch: 4 batch: 27 current batch loss: 0.07001800090074539\n",
            "epoch: 4 batch: 28 current batch loss: 0.07581470906734467\n",
            "epoch: 4 batch: 29 current batch loss: 0.07075600326061249\n",
            "epoch: 5 batch: 0 current batch loss: 0.06231781840324402\n",
            "epoch: 5 batch: 1 current batch loss: 0.06980550289154053\n",
            "epoch: 5 batch: 2 current batch loss: 0.05593598634004593\n",
            "epoch: 5 batch: 3 current batch loss: 0.060811568051576614\n",
            "epoch: 5 batch: 4 current batch loss: 0.05226042494177818\n",
            "epoch: 5 batch: 5 current batch loss: 0.06464767456054688\n",
            "epoch: 5 batch: 6 current batch loss: 0.06872551143169403\n",
            "epoch: 5 batch: 7 current batch loss: 0.05923584848642349\n",
            "epoch: 5 batch: 8 current batch loss: 0.055726222693920135\n",
            "epoch: 5 batch: 9 current batch loss: 0.06287980824708939\n",
            "epoch: 5 batch: 10 current batch loss: 0.05969584733247757\n",
            "epoch: 5 batch: 11 current batch loss: 0.0618547685444355\n",
            "epoch: 5 batch: 12 current batch loss: 0.06552038341760635\n",
            "epoch: 5 batch: 13 current batch loss: 0.06110818684101105\n",
            "epoch: 5 batch: 14 current batch loss: 0.0613178014755249\n",
            "epoch: 5 batch: 15 current batch loss: 0.05759967118501663\n",
            "epoch: 5 batch: 16 current batch loss: 0.04715157300233841\n",
            "epoch: 5 batch: 17 current batch loss: 0.04922255873680115\n",
            "epoch: 5 batch: 18 current batch loss: 0.04907716438174248\n",
            "epoch: 5 batch: 19 current batch loss: 0.06220421940088272\n",
            "epoch: 5 batch: 20 current batch loss: 0.05249279737472534\n",
            "epoch: 5 batch: 21 current batch loss: 0.06053897738456726\n",
            "epoch: 5 batch: 22 current batch loss: 0.0633983463048935\n",
            "epoch: 5 batch: 23 current batch loss: 0.058068182319402695\n",
            "epoch: 5 batch: 24 current batch loss: 0.05967125669121742\n",
            "epoch: 5 batch: 25 current batch loss: 0.05253808572888374\n",
            "epoch: 5 batch: 26 current batch loss: 0.05844299495220184\n",
            "epoch: 5 batch: 27 current batch loss: 0.05399375036358833\n",
            "epoch: 5 batch: 28 current batch loss: 0.06236659735441208\n",
            "epoch: 5 batch: 29 current batch loss: 0.05434166640043259\n",
            "epoch: 6 batch: 0 current batch loss: 0.053723156452178955\n",
            "epoch: 6 batch: 1 current batch loss: 0.060641806572675705\n",
            "epoch: 6 batch: 2 current batch loss: 0.05959382653236389\n",
            "epoch: 6 batch: 3 current batch loss: 0.04056302458047867\n",
            "epoch: 6 batch: 4 current batch loss: 0.04552658647298813\n",
            "epoch: 6 batch: 5 current batch loss: 0.04632197320461273\n",
            "epoch: 6 batch: 6 current batch loss: 0.04016193747520447\n",
            "epoch: 6 batch: 7 current batch loss: 0.04614948481321335\n",
            "epoch: 6 batch: 8 current batch loss: 0.04312482103705406\n",
            "epoch: 6 batch: 9 current batch loss: 0.0459851436316967\n",
            "epoch: 6 batch: 10 current batch loss: 0.04740350320935249\n",
            "epoch: 6 batch: 11 current batch loss: 0.05696882680058479\n",
            "epoch: 6 batch: 12 current batch loss: 0.047358326613903046\n",
            "epoch: 6 batch: 13 current batch loss: 0.04017462208867073\n",
            "epoch: 6 batch: 14 current batch loss: 0.03809357434511185\n",
            "epoch: 6 batch: 15 current batch loss: 0.04774531349539757\n",
            "epoch: 6 batch: 16 current batch loss: 0.04111089929938316\n",
            "epoch: 6 batch: 17 current batch loss: 0.0337299145758152\n",
            "epoch: 6 batch: 18 current batch loss: 0.050995588302612305\n",
            "epoch: 6 batch: 19 current batch loss: 0.04787440225481987\n",
            "epoch: 6 batch: 20 current batch loss: 0.060990866273641586\n",
            "epoch: 6 batch: 21 current batch loss: 0.04339959844946861\n",
            "epoch: 6 batch: 22 current batch loss: 0.055005837231874466\n",
            "epoch: 6 batch: 23 current batch loss: 0.04584961012005806\n",
            "epoch: 6 batch: 24 current batch loss: 0.03896031156182289\n",
            "epoch: 6 batch: 25 current batch loss: 0.03862282261252403\n",
            "epoch: 6 batch: 26 current batch loss: 0.0402679443359375\n",
            "epoch: 6 batch: 27 current batch loss: 0.04336551949381828\n",
            "epoch: 6 batch: 28 current batch loss: 0.048349879682064056\n",
            "epoch: 6 batch: 29 current batch loss: 0.05814381688833237\n",
            "epoch: 7 batch: 0 current batch loss: 0.032779354602098465\n",
            "epoch: 7 batch: 1 current batch loss: 0.04101430997252464\n",
            "epoch: 7 batch: 2 current batch loss: 0.03455575555562973\n",
            "epoch: 7 batch: 3 current batch loss: 0.04043373838067055\n",
            "epoch: 7 batch: 4 current batch loss: 0.048910483717918396\n",
            "epoch: 7 batch: 5 current batch loss: 0.03531543165445328\n",
            "epoch: 7 batch: 6 current batch loss: 0.033723343163728714\n",
            "epoch: 7 batch: 7 current batch loss: 0.035615988075733185\n",
            "epoch: 7 batch: 8 current batch loss: 0.03227042406797409\n",
            "epoch: 7 batch: 9 current batch loss: 0.034563541412353516\n",
            "epoch: 7 batch: 10 current batch loss: 0.036881446838378906\n",
            "epoch: 7 batch: 11 current batch loss: 0.030246952548623085\n",
            "epoch: 7 batch: 12 current batch loss: 0.03554229810833931\n",
            "epoch: 7 batch: 13 current batch loss: 0.04138792306184769\n",
            "epoch: 7 batch: 14 current batch loss: 0.031763892620801926\n",
            "epoch: 7 batch: 15 current batch loss: 0.03880125284194946\n",
            "epoch: 7 batch: 16 current batch loss: 0.04112938791513443\n",
            "epoch: 7 batch: 17 current batch loss: 0.03132123872637749\n",
            "epoch: 7 batch: 18 current batch loss: 0.03688286989927292\n",
            "epoch: 7 batch: 19 current batch loss: 0.0352998748421669\n",
            "epoch: 7 batch: 20 current batch loss: 0.03621426597237587\n",
            "epoch: 7 batch: 21 current batch loss: 0.03126569464802742\n",
            "epoch: 7 batch: 22 current batch loss: 0.03580433502793312\n",
            "epoch: 7 batch: 23 current batch loss: 0.037781283259391785\n",
            "epoch: 7 batch: 24 current batch loss: 0.035265885293483734\n",
            "epoch: 7 batch: 25 current batch loss: 0.040342140942811966\n",
            "epoch: 7 batch: 26 current batch loss: 0.03966236487030983\n",
            "epoch: 7 batch: 27 current batch loss: 0.03671066090464592\n",
            "epoch: 7 batch: 28 current batch loss: 0.04186185076832771\n",
            "epoch: 7 batch: 29 current batch loss: 0.06231793016195297\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
      "metadata": {
        "id": "9b257c80-3965-4bdf-9e47-0da1be6891de"
      },
      "source": [
        "### Answers to task #5\n",
        "\n",
        "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
        "\n",
        "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "e6d2a2b7-72eb-4fa8-e6f7-741d13ea6a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.3059325218200684 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.078433036804199 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 1.6184415817260742 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 1.1510072946548462 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 1.0691945552825928 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 1.0500121116638184 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 1.1987625360488892 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 0.8782199621200562 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 0.7880807518959045 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 0.8341243267059326 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 0.7004535794258118 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 0.6361493468284607 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 0.5678898692131042 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 0.581855297088623 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 0.531477689743042 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 0.5078736543655396 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 0.5306468605995178 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 0.45890453457832336 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 0.4300965368747711 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 0.4003583788871765 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 0.3830206096172333 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 0.39846616983413696 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 0.3620540499687195 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 0.34950193762779236 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 0.34204888343811035 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 0.3205997049808502 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 0.33825093507766724 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 0.30562376976013184 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 0.30770108103752136 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 0.3080151081085205 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.29788047075271606 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.3049071431159973 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.26596537232398987 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.291525274515152 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.24922436475753784 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.2484811544418335 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.24335666000843048 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.25989699363708496 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.22954218089580536 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.1911209374666214 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.21126791834831238 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.2097291201353073 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.20598657429218292 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.19771063327789307 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.20662419497966766 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.20541176199913025 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.2005021721124649 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.1826040893793106 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.20517157018184662 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.19901108741760254 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.195700541138649 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.2084432989358902 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.15138912200927734 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.18892373144626617 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.18968072533607483 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.15793469548225403 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.18036450445652008 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.17648513615131378 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.20011945068836212 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.18357691168785095 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.15874676406383514 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.1476633995771408 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.1472112089395523 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.16292977333068848 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.1331537812948227 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.17516463994979858 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.12136930227279663 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.1464722603559494 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.1509532928466797 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.13399654626846313 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.13549654185771942 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.12068046629428864 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.1546253263950348 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.13728614151477814 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.15493354201316833 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.1285271793603897 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.14151129126548767 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.13037660717964172 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.15311357378959656 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.1321955919265747 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.14150284230709076 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.1334168016910553 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.11568764597177505 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.11911128461360931 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.12667281925678253 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.11614558100700378 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.12799882888793945 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.12162846326828003 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.14042091369628906 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.12094148993492126 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.10906904190778732 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.110391765832901 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.09687510132789612 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.10989139974117279 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.09538813680410385 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.11393148452043533 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.09724251925945282 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.11025341600179672 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.12924498319625854 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.09202351421117783 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.11582359671592712 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.11687424778938293 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.10687515139579773 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.10648909211158752 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.09739536792039871 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.08308915048837662 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.12605397403240204 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.09593155235052109 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.09039100259542465 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.09458599984645844 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.1064070612192154 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.10252245515584946 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.09047035127878189 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.10756368190050125 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.10174023360013962 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.10246612876653671 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.10607155412435532 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.1002168357372284 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.10178341716527939 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.08242736011743546 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.07826447486877441 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.08772668242454529 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.0826166421175003 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.07506083697080612 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.07044561952352524 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.07922574877738953 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.07774045318365097 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.07198948413133621 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.0760265365242958 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.07608450949192047 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.08511201292276382 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.08213972300291061 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.08070793002843857 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.09108753502368927 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.06672373414039612 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.08254119753837585 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.091437429189682 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.08258271962404251 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.0677393227815628 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.07005277276039124 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.07791078090667725 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.07222810387611389 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.07844460755586624 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.08121076971292496 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.0802808627486229 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.07805661112070084 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.06368338316679001 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.08507874608039856 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.07528474926948547 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.09567524492740631 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.07087220251560211 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.06066206470131874 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.06602876633405685 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.07292621582746506 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.08273307979106903 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.06050393730401993 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.05968257039785385 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.05277504026889801 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.0606498196721077 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.05688918009400368 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.061125464737415314 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.07087774574756622 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.05871577933430672 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.06995333731174469 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.0577441044151783 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.05925305187702179 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.06854677200317383 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.0673908069729805 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.06451147049665451 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.06782902777194977 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.062487486749887466 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.05119457468390465 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.05508594959974289 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.06124964728951454 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.06168501451611519 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.06177786737680435 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.0677325502038002 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.052858103066682816 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.06500200182199478 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.05601238086819649 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.04459363967180252 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.07384946197271347 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.06021418049931526 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.05583784356713295 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.05212431773543358 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.05488608777523041 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.05645477771759033 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.04649018123745918 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.06574410200119019 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.051824528723955154 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.058361656963825226 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.05263660103082657 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.05104590579867363 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.048750266432762146 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.048998940736055374 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.05268234759569168 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.052458636462688446 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.04725489392876625 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.07744546979665756 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.05069121718406677 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.04444977268576622 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.05754013732075691 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.050626832991838455 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.04440285637974739 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.04645752161741257 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.06324779242277145 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.0427546501159668 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.052854739129543304 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.05315973237156868 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.07128960639238358 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.04666142165660858 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.05244166776537895 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.051344133913517 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.03973182663321495 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.045138534158468246 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.040439169853925705 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.0431341677904129 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.04150645062327385 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.04318099841475487 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.03841568902134895 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.0436413399875164 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.0418243445456028 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.04585014283657074 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.04488484561443329 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.04480140656232834 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.045971859246492386 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.044046808034181595 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.036443084478378296 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.05320768058300018 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.049433887004852295 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.032034020870923996 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.04144757241010666 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.052124883979558945 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.046732690185308456 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.0477888360619545 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.036690935492515564 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.041175488382577896 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.054315198212862015 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.042962830513715744 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.058302685618400574 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0])\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        "\n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.03 even without a scheduler. Can you bring it under 0.02? Can you keep it under 0.02? The scheduler didn't help much. Maybe the proposed gamma was to low (0.9 only)? Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "**Some other schedulers you might want to experiment with:**\n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html)\n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "\n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "34b35875-0b90-4ad7-a592-bd560d34f103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9816\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "b93dde1f-7dd7-4a28-ee60-6a5f4d7e6ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9824\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}