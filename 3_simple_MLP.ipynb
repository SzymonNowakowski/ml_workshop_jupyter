{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/ml_workshop_jupyter/blob/main/3_simple_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers\n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*).\n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "### Answers to task #1\n",
        "A nonlinear component in-between the linear layers is essential:\n",
        "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network.\n",
        "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits from *zero* to *nine*. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "**You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).**\n",
        "\n",
        "# Mathematically oriented notation for the MLP\n",
        "\n",
        "A three layer perceptron we will work further with in this workshop can be defined in a mathematematically apealing way as\n",
        "\n",
        "$f:\\mathbb{R}^{28\\cdot 28 + D} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
        "\n",
        "$f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4$,\n",
        "\n",
        "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and $\\left[ \\cdot \\right]_+$ is taking a positive part, which is another notation for ReLU.\n",
        "\n",
        "Note, that there is no nonlinear activation after the third layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n",
        "\n",
        "# Automatic gradient\n",
        "\n",
        "The [automatic gradient functionality covered in another workshop](https://github.com/SzymonNowakowski/ml_workshop_jupyter/blob/main/1_computational_graph.ipynb) will be used to automatically calculate\n",
        "gradient of\n",
        "$loss \\left(f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right), y_i\\right)$ for the training set $\\left(x_i, y_i \\right)_{i=1, \\ldots, N}$\n",
        "with respect to each component of $W_i$ and $b_i$ tensors.\n",
        "\n",
        "The training set will be batched, but it is just a technical detail.\n",
        "\n",
        "# Loss\n",
        "\n",
        "For the loss function we will use a crossentropy loss directly from PyTorch functional library `torch.nn.functional`. You can read more about this loss [in PyTorch documentation about CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) or generally in [`torch.nn.functional` loss functions section](https://pytorch.org/docs/2.1/nn.functional.html#loss-functions).\n",
        "\n",
        "### Workshop dedicated to loss functions\n",
        "\n",
        "**Also, [there is a workshop dedicated to loss functions](https://github.com/SzymonNowakowski/ml_workshop_jupyter/blob/main/2_loss_functions.ipynb)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68",
      "metadata": {
        "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
        "outputId": "e31f7b8a-656d-443f-ab7d-c803eb91bd21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 126362645.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27700821.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 77280300.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18993548.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "ec8f00bb-54a6-41d7-dc2f-382c1bdf8037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "e24fceef-a084-4b41-d8e2-650068f931c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "1fb6a80c-2539-4e75-beff-61b7089c169d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:*\n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
      "metadata": {
        "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
        "outputId": "8e633113-2769-4d6b-ef2a-b900d0bb47d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1306604762738429, 0.30810780385646264)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "27d0f9f0-615a-47cc-be8c-578e04e6df91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([5, 0, 9,  ..., 2, 1, 2])\n",
            "1 -th batch labels : tensor([1, 3, 8,  ..., 0, 5, 1])\n",
            "2 -th batch labels : tensor([1, 0, 2,  ..., 3, 7, 1])\n",
            "3 -th batch labels : tensor([1, 1, 2,  ..., 6, 6, 8])\n",
            "4 -th batch labels : tensor([6, 9, 9,  ..., 4, 1, 7])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch.\n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
      "metadata": {
        "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
        "outputId": "d74d0a1c-49bd-4aae-d2a4-55d35772e1d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
      "metadata": {
        "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168"
      },
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
        "                                          #that output from a preceding layer serves as input to the next layer\n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "9cfef271-e9fa-41d5-a5a6-a8af5f20d376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.306473731994629\n",
            "epoch: 0 batch: 1 current batch loss: 2.1020095348358154\n",
            "epoch: 0 batch: 2 current batch loss: 1.656168818473816\n",
            "epoch: 0 batch: 3 current batch loss: 1.1334954500198364\n",
            "epoch: 0 batch: 4 current batch loss: 1.0116450786590576\n",
            "epoch: 0 batch: 5 current batch loss: 1.3850033283233643\n",
            "epoch: 0 batch: 6 current batch loss: 1.2654603719711304\n",
            "epoch: 0 batch: 7 current batch loss: 1.0417665243148804\n",
            "epoch: 0 batch: 8 current batch loss: 1.028128743171692\n",
            "epoch: 0 batch: 9 current batch loss: 0.7316476702690125\n",
            "epoch: 0 batch: 10 current batch loss: 0.6175505518913269\n",
            "epoch: 0 batch: 11 current batch loss: 0.6861860752105713\n",
            "epoch: 0 batch: 12 current batch loss: 0.673275887966156\n",
            "epoch: 0 batch: 13 current batch loss: 0.6559863090515137\n",
            "epoch: 0 batch: 14 current batch loss: 0.641333818435669\n",
            "epoch: 0 batch: 15 current batch loss: 0.5830376148223877\n",
            "epoch: 0 batch: 16 current batch loss: 0.5123422741889954\n",
            "epoch: 0 batch: 17 current batch loss: 0.4765959084033966\n",
            "epoch: 0 batch: 18 current batch loss: 0.4710223376750946\n",
            "epoch: 0 batch: 19 current batch loss: 0.4926372170448303\n",
            "epoch: 0 batch: 20 current batch loss: 0.45660439133644104\n",
            "epoch: 0 batch: 21 current batch loss: 0.4581763744354248\n",
            "epoch: 0 batch: 22 current batch loss: 0.4092506468296051\n",
            "epoch: 0 batch: 23 current batch loss: 0.35327771306037903\n",
            "epoch: 0 batch: 24 current batch loss: 0.3693143129348755\n",
            "epoch: 0 batch: 25 current batch loss: 0.35373640060424805\n",
            "epoch: 0 batch: 26 current batch loss: 0.3767617642879486\n",
            "epoch: 0 batch: 27 current batch loss: 0.3382132947444916\n",
            "epoch: 0 batch: 28 current batch loss: 0.3375028967857361\n",
            "epoch: 0 batch: 29 current batch loss: 0.36021193861961365\n",
            "epoch: 1 batch: 0 current batch loss: 0.27273067831993103\n",
            "epoch: 1 batch: 1 current batch loss: 0.29458707571029663\n",
            "epoch: 1 batch: 2 current batch loss: 0.29106009006500244\n",
            "epoch: 1 batch: 3 current batch loss: 0.27267807722091675\n",
            "epoch: 1 batch: 4 current batch loss: 0.30670085549354553\n",
            "epoch: 1 batch: 5 current batch loss: 0.2597983777523041\n",
            "epoch: 1 batch: 6 current batch loss: 0.26776108145713806\n",
            "epoch: 1 batch: 7 current batch loss: 0.23566211760044098\n",
            "epoch: 1 batch: 8 current batch loss: 0.2651456594467163\n",
            "epoch: 1 batch: 9 current batch loss: 0.22707171738147736\n",
            "epoch: 1 batch: 10 current batch loss: 0.22716695070266724\n",
            "epoch: 1 batch: 11 current batch loss: 0.2201392650604248\n",
            "epoch: 1 batch: 12 current batch loss: 0.22006291151046753\n",
            "epoch: 1 batch: 13 current batch loss: 0.23381856083869934\n",
            "epoch: 1 batch: 14 current batch loss: 0.22448672354221344\n",
            "epoch: 1 batch: 15 current batch loss: 0.2030845284461975\n",
            "epoch: 1 batch: 16 current batch loss: 0.22523000836372375\n",
            "epoch: 1 batch: 17 current batch loss: 0.2130925953388214\n",
            "epoch: 1 batch: 18 current batch loss: 0.2086336761713028\n",
            "epoch: 1 batch: 19 current batch loss: 0.1996132731437683\n",
            "epoch: 1 batch: 20 current batch loss: 0.18020376563072205\n",
            "epoch: 1 batch: 21 current batch loss: 0.18850156664848328\n",
            "epoch: 1 batch: 22 current batch loss: 0.18724459409713745\n",
            "epoch: 1 batch: 23 current batch loss: 0.19194509088993073\n",
            "epoch: 1 batch: 24 current batch loss: 0.18411709368228912\n",
            "epoch: 1 batch: 25 current batch loss: 0.1876581460237503\n",
            "epoch: 1 batch: 26 current batch loss: 0.17896762490272522\n",
            "epoch: 1 batch: 27 current batch loss: 0.1866973638534546\n",
            "epoch: 1 batch: 28 current batch loss: 0.1783570796251297\n",
            "epoch: 1 batch: 29 current batch loss: 0.20824357867240906\n",
            "epoch: 2 batch: 0 current batch loss: 0.1391218900680542\n",
            "epoch: 2 batch: 1 current batch loss: 0.16141340136528015\n",
            "epoch: 2 batch: 2 current batch loss: 0.1289500743150711\n",
            "epoch: 2 batch: 3 current batch loss: 0.15117375552654266\n",
            "epoch: 2 batch: 4 current batch loss: 0.14609774947166443\n",
            "epoch: 2 batch: 5 current batch loss: 0.1461821049451828\n",
            "epoch: 2 batch: 6 current batch loss: 0.12790121138095856\n",
            "epoch: 2 batch: 7 current batch loss: 0.13941310346126556\n",
            "epoch: 2 batch: 8 current batch loss: 0.15812964737415314\n",
            "epoch: 2 batch: 9 current batch loss: 0.12197281420230865\n",
            "epoch: 2 batch: 10 current batch loss: 0.14145012199878693\n",
            "epoch: 2 batch: 11 current batch loss: 0.14358951151371002\n",
            "epoch: 2 batch: 12 current batch loss: 0.14115898311138153\n",
            "epoch: 2 batch: 13 current batch loss: 0.13934990763664246\n",
            "epoch: 2 batch: 14 current batch loss: 0.12477260828018188\n",
            "epoch: 2 batch: 15 current batch loss: 0.13834425806999207\n",
            "epoch: 2 batch: 16 current batch loss: 0.13682147860527039\n",
            "epoch: 2 batch: 17 current batch loss: 0.1302863359451294\n",
            "epoch: 2 batch: 18 current batch loss: 0.14075535535812378\n",
            "epoch: 2 batch: 19 current batch loss: 0.11247314512729645\n",
            "epoch: 2 batch: 20 current batch loss: 0.1388636976480484\n",
            "epoch: 2 batch: 21 current batch loss: 0.1424712836742401\n",
            "epoch: 2 batch: 22 current batch loss: 0.15349555015563965\n",
            "epoch: 2 batch: 23 current batch loss: 0.16671016812324524\n",
            "epoch: 2 batch: 24 current batch loss: 0.1276826560497284\n",
            "epoch: 2 batch: 25 current batch loss: 0.11658415198326111\n",
            "epoch: 2 batch: 26 current batch loss: 0.1341550201177597\n",
            "epoch: 2 batch: 27 current batch loss: 0.10587179660797119\n",
            "epoch: 2 batch: 28 current batch loss: 0.12366461008787155\n",
            "epoch: 2 batch: 29 current batch loss: 0.1698993742465973\n",
            "epoch: 3 batch: 0 current batch loss: 0.10759977251291275\n",
            "epoch: 3 batch: 1 current batch loss: 0.10910583287477493\n",
            "epoch: 3 batch: 2 current batch loss: 0.10623567551374435\n",
            "epoch: 3 batch: 3 current batch loss: 0.1160176545381546\n",
            "epoch: 3 batch: 4 current batch loss: 0.09647156298160553\n",
            "epoch: 3 batch: 5 current batch loss: 0.11490964889526367\n",
            "epoch: 3 batch: 6 current batch loss: 0.09894905239343643\n",
            "epoch: 3 batch: 7 current batch loss: 0.09654834866523743\n",
            "epoch: 3 batch: 8 current batch loss: 0.10289321839809418\n",
            "epoch: 3 batch: 9 current batch loss: 0.08743437379598618\n",
            "epoch: 3 batch: 10 current batch loss: 0.0976576954126358\n",
            "epoch: 3 batch: 11 current batch loss: 0.1048944741487503\n",
            "epoch: 3 batch: 12 current batch loss: 0.10128451138734818\n",
            "epoch: 3 batch: 13 current batch loss: 0.10029521584510803\n",
            "epoch: 3 batch: 14 current batch loss: 0.0978885069489479\n",
            "epoch: 3 batch: 15 current batch loss: 0.09204568713903427\n",
            "epoch: 3 batch: 16 current batch loss: 0.09472011029720306\n",
            "epoch: 3 batch: 17 current batch loss: 0.0824209600687027\n",
            "epoch: 3 batch: 18 current batch loss: 0.10716854780912399\n",
            "epoch: 3 batch: 19 current batch loss: 0.09979420155286789\n",
            "epoch: 3 batch: 20 current batch loss: 0.11993430554866791\n",
            "epoch: 3 batch: 21 current batch loss: 0.08951716125011444\n",
            "epoch: 3 batch: 22 current batch loss: 0.10821559280157089\n",
            "epoch: 3 batch: 23 current batch loss: 0.10281434655189514\n",
            "epoch: 3 batch: 24 current batch loss: 0.11201488971710205\n",
            "epoch: 3 batch: 25 current batch loss: 0.11647764593362808\n",
            "epoch: 3 batch: 26 current batch loss: 0.08935191482305527\n",
            "epoch: 3 batch: 27 current batch loss: 0.10189978778362274\n",
            "epoch: 3 batch: 28 current batch loss: 0.08491639792919159\n",
            "epoch: 3 batch: 29 current batch loss: 0.10130667686462402\n",
            "epoch: 4 batch: 0 current batch loss: 0.07875325530767441\n",
            "epoch: 4 batch: 1 current batch loss: 0.0784531831741333\n",
            "epoch: 4 batch: 2 current batch loss: 0.06878173351287842\n",
            "epoch: 4 batch: 3 current batch loss: 0.09301476180553436\n",
            "epoch: 4 batch: 4 current batch loss: 0.08973155170679092\n",
            "epoch: 4 batch: 5 current batch loss: 0.10148512572050095\n",
            "epoch: 4 batch: 6 current batch loss: 0.07016389071941376\n",
            "epoch: 4 batch: 7 current batch loss: 0.08812836557626724\n",
            "epoch: 4 batch: 8 current batch loss: 0.07375874370336533\n",
            "epoch: 4 batch: 9 current batch loss: 0.07621587812900543\n",
            "epoch: 4 batch: 10 current batch loss: 0.07985188066959381\n",
            "epoch: 4 batch: 11 current batch loss: 0.07454418390989304\n",
            "epoch: 4 batch: 12 current batch loss: 0.06789681315422058\n",
            "epoch: 4 batch: 13 current batch loss: 0.06687307357788086\n",
            "epoch: 4 batch: 14 current batch loss: 0.09226760268211365\n",
            "epoch: 4 batch: 15 current batch loss: 0.0799325555562973\n",
            "epoch: 4 batch: 16 current batch loss: 0.08226516097784042\n",
            "epoch: 4 batch: 17 current batch loss: 0.07088697701692581\n",
            "epoch: 4 batch: 18 current batch loss: 0.08345916867256165\n",
            "epoch: 4 batch: 19 current batch loss: 0.08145663887262344\n",
            "epoch: 4 batch: 20 current batch loss: 0.08346972614526749\n",
            "epoch: 4 batch: 21 current batch loss: 0.06509578973054886\n",
            "epoch: 4 batch: 22 current batch loss: 0.06511866301298141\n",
            "epoch: 4 batch: 23 current batch loss: 0.0679636225104332\n",
            "epoch: 4 batch: 24 current batch loss: 0.07413104176521301\n",
            "epoch: 4 batch: 25 current batch loss: 0.08209000527858734\n",
            "epoch: 4 batch: 26 current batch loss: 0.07618160545825958\n",
            "epoch: 4 batch: 27 current batch loss: 0.07584896683692932\n",
            "epoch: 4 batch: 28 current batch loss: 0.07607332617044449\n",
            "epoch: 4 batch: 29 current batch loss: 0.07827727496623993\n",
            "epoch: 5 batch: 0 current batch loss: 0.05990516394376755\n",
            "epoch: 5 batch: 1 current batch loss: 0.050840940326452255\n",
            "epoch: 5 batch: 2 current batch loss: 0.05560240149497986\n",
            "epoch: 5 batch: 3 current batch loss: 0.06646699458360672\n",
            "epoch: 5 batch: 4 current batch loss: 0.07158607244491577\n",
            "epoch: 5 batch: 5 current batch loss: 0.05334020033478737\n",
            "epoch: 5 batch: 6 current batch loss: 0.05601463466882706\n",
            "epoch: 5 batch: 7 current batch loss: 0.0518123023211956\n",
            "epoch: 5 batch: 8 current batch loss: 0.06645439565181732\n",
            "epoch: 5 batch: 9 current batch loss: 0.0631098672747612\n",
            "epoch: 5 batch: 10 current batch loss: 0.0577155202627182\n",
            "epoch: 5 batch: 11 current batch loss: 0.051644790917634964\n",
            "epoch: 5 batch: 12 current batch loss: 0.0564572736620903\n",
            "epoch: 5 batch: 13 current batch loss: 0.05382203310728073\n",
            "epoch: 5 batch: 14 current batch loss: 0.08440054208040237\n",
            "epoch: 5 batch: 15 current batch loss: 0.0674235075712204\n",
            "epoch: 5 batch: 16 current batch loss: 0.08059868961572647\n",
            "epoch: 5 batch: 17 current batch loss: 0.0635760948061943\n",
            "epoch: 5 batch: 18 current batch loss: 0.06282046437263489\n",
            "epoch: 5 batch: 19 current batch loss: 0.05921085923910141\n",
            "epoch: 5 batch: 20 current batch loss: 0.06975637376308441\n",
            "epoch: 5 batch: 21 current batch loss: 0.07244718074798584\n",
            "epoch: 5 batch: 22 current batch loss: 0.05767689645290375\n",
            "epoch: 5 batch: 23 current batch loss: 0.06452003866434097\n",
            "epoch: 5 batch: 24 current batch loss: 0.05508129671216011\n",
            "epoch: 5 batch: 25 current batch loss: 0.06702569127082825\n",
            "epoch: 5 batch: 26 current batch loss: 0.047620225697755814\n",
            "epoch: 5 batch: 27 current batch loss: 0.06811504811048508\n",
            "epoch: 5 batch: 28 current batch loss: 0.05260980874300003\n",
            "epoch: 5 batch: 29 current batch loss: 0.06370978057384491\n",
            "epoch: 6 batch: 0 current batch loss: 0.05425562709569931\n",
            "epoch: 6 batch: 1 current batch loss: 0.049704525619745255\n",
            "epoch: 6 batch: 2 current batch loss: 0.04416394233703613\n",
            "epoch: 6 batch: 3 current batch loss: 0.05586586892604828\n",
            "epoch: 6 batch: 4 current batch loss: 0.059585798531770706\n",
            "epoch: 6 batch: 5 current batch loss: 0.04100386053323746\n",
            "epoch: 6 batch: 6 current batch loss: 0.04382266849279404\n",
            "epoch: 6 batch: 7 current batch loss: 0.04693327471613884\n",
            "epoch: 6 batch: 8 current batch loss: 0.04560127109289169\n",
            "epoch: 6 batch: 9 current batch loss: 0.046057626605033875\n",
            "epoch: 6 batch: 10 current batch loss: 0.0485488623380661\n",
            "epoch: 6 batch: 11 current batch loss: 0.03809409961104393\n",
            "epoch: 6 batch: 12 current batch loss: 0.048281230032444\n",
            "epoch: 6 batch: 13 current batch loss: 0.051183514297008514\n",
            "epoch: 6 batch: 14 current batch loss: 0.0352177657186985\n",
            "epoch: 6 batch: 15 current batch loss: 0.04597095772624016\n",
            "epoch: 6 batch: 16 current batch loss: 0.04607096314430237\n",
            "epoch: 6 batch: 17 current batch loss: 0.04991193860769272\n",
            "epoch: 6 batch: 18 current batch loss: 0.04127921536564827\n",
            "epoch: 6 batch: 19 current batch loss: 0.051847025752067566\n",
            "epoch: 6 batch: 20 current batch loss: 0.044184524565935135\n",
            "epoch: 6 batch: 21 current batch loss: 0.04140666127204895\n",
            "epoch: 6 batch: 22 current batch loss: 0.052541911602020264\n",
            "epoch: 6 batch: 23 current batch loss: 0.04412093386054039\n",
            "epoch: 6 batch: 24 current batch loss: 0.05352521315217018\n",
            "epoch: 6 batch: 25 current batch loss: 0.04900972172617912\n",
            "epoch: 6 batch: 26 current batch loss: 0.04717082902789116\n",
            "epoch: 6 batch: 27 current batch loss: 0.04862714931368828\n",
            "epoch: 6 batch: 28 current batch loss: 0.05425042286515236\n",
            "epoch: 6 batch: 29 current batch loss: 0.07043835520744324\n",
            "epoch: 7 batch: 0 current batch loss: 0.044721826910972595\n",
            "epoch: 7 batch: 1 current batch loss: 0.04768151044845581\n",
            "epoch: 7 batch: 2 current batch loss: 0.03701778128743172\n",
            "epoch: 7 batch: 3 current batch loss: 0.03641963750123978\n",
            "epoch: 7 batch: 4 current batch loss: 0.04065984860062599\n",
            "epoch: 7 batch: 5 current batch loss: 0.035680513828992844\n",
            "epoch: 7 batch: 6 current batch loss: 0.04132470488548279\n",
            "epoch: 7 batch: 7 current batch loss: 0.05304460600018501\n",
            "epoch: 7 batch: 8 current batch loss: 0.04335761070251465\n",
            "epoch: 7 batch: 9 current batch loss: 0.042912714183330536\n",
            "epoch: 7 batch: 10 current batch loss: 0.04357364773750305\n",
            "epoch: 7 batch: 11 current batch loss: 0.036303479224443436\n",
            "epoch: 7 batch: 12 current batch loss: 0.040057241916656494\n",
            "epoch: 7 batch: 13 current batch loss: 0.04273245111107826\n",
            "epoch: 7 batch: 14 current batch loss: 0.03946946933865547\n",
            "epoch: 7 batch: 15 current batch loss: 0.03887329250574112\n",
            "epoch: 7 batch: 16 current batch loss: 0.04862058162689209\n",
            "epoch: 7 batch: 17 current batch loss: 0.03800531476736069\n",
            "epoch: 7 batch: 18 current batch loss: 0.04642253741621971\n",
            "epoch: 7 batch: 19 current batch loss: 0.038069821894168854\n",
            "epoch: 7 batch: 20 current batch loss: 0.03395755961537361\n",
            "epoch: 7 batch: 21 current batch loss: 0.04082105681300163\n",
            "epoch: 7 batch: 22 current batch loss: 0.0429343618452549\n",
            "epoch: 7 batch: 23 current batch loss: 0.041075922548770905\n",
            "epoch: 7 batch: 24 current batch loss: 0.030453991144895554\n",
            "epoch: 7 batch: 25 current batch loss: 0.03451169654726982\n",
            "epoch: 7 batch: 26 current batch loss: 0.04146371781826019\n",
            "epoch: 7 batch: 27 current batch loss: 0.029059072956442833\n",
            "epoch: 7 batch: 28 current batch loss: 0.03751339018344879\n",
            "epoch: 7 batch: 29 current batch loss: 0.02943321503698826\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
      "metadata": {
        "id": "9b257c80-3965-4bdf-9e47-0da1be6891de"
      },
      "source": [
        "### Answers to task #5\n",
        "\n",
        "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
        "\n",
        "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "c3c406f8-540d-4c24-cc0d-8172e302d5ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.304553508758545 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.078700304031372 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 1.6457241773605347 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 1.1277457475662231 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 0.980464518070221 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 1.1679720878601074 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 1.229499340057373 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 1.139792799949646 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 1.018923044204712 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 0.6823866963386536 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 0.6440799236297607 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 0.7058770060539246 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 0.6701802611351013 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 0.6861715316772461 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 0.6120184659957886 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 0.5329363942146301 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 0.5130503177642822 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 0.47115010023117065 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 0.4395769536495209 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 0.43074363470077515 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 0.42871883511543274 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 0.39322564005851746 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 0.37245669960975647 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 0.3846038281917572 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 0.3101191222667694 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 0.37170571088790894 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 0.36616382002830505 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 0.3345423638820648 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 0.30551373958587646 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 0.295259565114975 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.28794750571250916 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.27777042984962463 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.23297119140625 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.2408171147108078 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.28146541118621826 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.24600441753864288 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.24017015099525452 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.2469213753938675 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.23800145089626312 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.22588539123535156 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.2442796677350998 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.2500643730163574 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.24549894034862518 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.24477626383304596 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.24017927050590515 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.21383824944496155 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.21070702373981476 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.2005796730518341 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.22981633245944977 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.1679842174053192 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.20857080817222595 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.18919651210308075 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.19502004981040955 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.18542148172855377 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.2027326077222824 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.2064151018857956 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.19346170127391815 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.17050397396087646 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.16533124446868896 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.17773456871509552 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.16821275651454926 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.14730915427207947 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.1630721390247345 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.16553154587745667 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.15397417545318604 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.14393433928489685 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.1296035498380661 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.17969433963298798 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.15133066475391388 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.17099568247795105 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.1456504464149475 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.1365000158548355 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.14502181112766266 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.13290433585643768 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.1615716964006424 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.14946497976779938 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.12417658418416977 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.13815899193286896 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.13263557851314545 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.13591092824935913 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.14374342560768127 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.11781061440706253 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.13493114709854126 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.1292215883731842 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.1546037495136261 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.11394058167934418 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.13190802931785583 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.12633371353149414 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.12217835336923599 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.12734586000442505 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.1182987242937088 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.09513203054666519 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.10843196511268616 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.10336942225694656 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.11119663715362549 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.10403497517108917 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.10275588929653168 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.12896734476089478 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.11146311461925507 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.10284406691789627 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.11053229123353958 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.10773501545190811 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.098698191344738 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.10057111084461212 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.10844521224498749 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.1084512397646904 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.11788565665483475 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.10413843393325806 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.08689538389444351 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.10984157770872116 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.0870751366019249 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.10498666763305664 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.11682360619306564 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.08673054724931717 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.10591889917850494 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.10680034011602402 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.10401496291160583 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.08175132423639297 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.08578944951295853 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.10840822756290436 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.09047430753707886 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.07653436809778214 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.08074215799570084 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.0740131363272667 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.07245513051748276 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.09005721658468246 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.08458361029624939 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.09058824181556702 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.07907339185476303 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.07259267568588257 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.09017763286828995 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.08613692224025726 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.08523279428482056 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.07898936420679092 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.07725109159946442 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.08212003856897354 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.08512815833091736 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.08016651123762131 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.09403843432664871 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.07715987414121628 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.07671400159597397 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.07337090373039246 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.06558606028556824 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.08255728334188461 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.09212213009595871 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.07595084607601166 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.07664432376623154 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.08449029922485352 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.080937460064888 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.08156265318393707 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.06806599348783493 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.07721507549285889 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.06490325182676315 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.06390013545751572 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.07831742614507675 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.07975198328495026 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.062226876616477966 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.05048862472176552 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.06690710783004761 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.061973895877599716 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.06489786505699158 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.06441494822502136 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.064780093729496 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.07571491599082947 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.0636143684387207 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.07340101897716522 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.07511838525533676 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.06059757247567177 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.06355734169483185 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.05243676155805588 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.054323941469192505 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.06205645576119423 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.05650383606553078 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.059562698006629944 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.06623165309429169 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.05641709640622139 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.06562463939189911 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.05963215231895447 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.06700095534324646 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.06225970387458801 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.06765086948871613 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.05544067174196243 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.06270565092563629 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.06082712486386299 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.05658143386244774 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.04695352166891098 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.06038905680179596 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.04275880753993988 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.054501693695783615 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.04895348474383354 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.06449408084154129 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.05993379279971123 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.05609354376792908 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.051923129707574844 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.06426385790109634 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.06574183702468872 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.05769941583275795 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.050505854189395905 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.04892135038971901 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.04642610624432564 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.038870491087436676 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.05054554343223572 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.055808015167713165 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.0544646717607975 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.050940681248903275 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.04898879677057266 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.05616247281432152 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.05753867328166962 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.052821021527051926 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.06769043952226639 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.049217332154512405 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.05242787301540375 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.04378419369459152 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.04561987146735191 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.04109656438231468 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.03608071431517601 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.03840479254722595 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.047784414142370224 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.044544000178575516 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.050701603293418884 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.04108929634094238 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.040242403745651245 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.035581085830926895 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.047455862164497375 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.04052511230111122 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.058050598949193954 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.043091680854558945 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.05080445483326912 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.0429358184337616 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.05317182466387749 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.033645324409008026 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.04627208411693573 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.04552936181426048 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.04271238297224045 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.03875937685370445 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.04452722519636154 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.045172739773988724 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.0406351238489151 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.04623408615589142 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.055240511894226074 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0])\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        "\n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.03 even without a scheduler. Can you bring it under 0.02? Can you keep it under 0.02? The scheduler didn't help much. Maybe the proposed gamma was to low (0.9 only)? Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "**Some other schedulers you might want to experiment with:**\n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html)\n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "\n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "23dd9bed-ee18-4fd4-c8d5-45b531822d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9802\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "6d6aa07a-fe65-4836-9923-bbe8221ec784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9815\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler.\n",
        "\n",
        "\n",
        "# Moving computations to GPU\n",
        "\n",
        "In the code above we didn't move the computations to GPU. The first task is moving the computations to GPU. How do you do that?\n",
        "\n",
        "Ability to compute on GPU is called CUDA:  Compute Unified Device Architecture.\n",
        "\n",
        "First, we check if CUDA is available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "ehuPd4dXGRkO",
        "outputId": "d2c00ff9-3565-4007-b938-3e96741e63e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ehuPd4dXGRkO",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to make CUDA available if it is not?\n",
        "\n",
        "* On a local machine you must connect and properly configure a GPU card, it is out of scope of this workshop.\n",
        "\n",
        "* In Google Colab online, you click `Runtime` > `Change runtime` and change `Hardware acceleration` setting to `GPU`.\n",
        "\n",
        "* In Google Colab **Polish version**, you click `Środowisko wykonawcze` > `Zmień typ środowiska wykonawczego` and change `Akcelerator sprzętowy` setting to `GPU`.\n",
        "\n",
        "Once we have CUDA (or not: this case should be handled in the code, too), we are ready to determine the computation device. Note that it is just a character string:\n"
      ],
      "metadata": {
        "id": "bKCTsxL2GXva"
      },
      "id": "bKCTsxL2GXva"
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HSvyKWniGc4V",
        "outputId": "a2a094b8-e760-4482-9d24-5ccf5c7878d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HSvyKWniGc4V",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we move our neural network model and each tensor with batch data to GPU. We can do it by passing the `device = device` argument during the model or tensor construction, or by calling a `to(device)` method on a model or a tensor:"
      ],
      "metadata": {
        "id": "EZaZF0NJHIRL"
      },
      "id": "EZaZF0NJHIRL"
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Linear(2048, 256, device = device),\n",
        "layer = torch.nn.Linear(2048, 256).to(device)"
      ],
      "metadata": {
        "id": "TPa7YYZWHMI0"
      },
      "id": "TPa7YYZWHMI0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your task #8\n",
        "\n",
        "Enable CUDA computations in all the code above:\n",
        "\n",
        "1. Check if CUDA is available before the MLP definition\n",
        "2. Add the device field to the constructor and pass it to all constructed layers\n",
        "3. Add the device argument when constructing MLP\n",
        "4. Add the device arguments to all tensors that get passed to MLP"
      ],
      "metadata": {
        "id": "xMovK3L5HMx6"
      },
      "id": "xMovK3L5HMx6"
    },
    {
      "cell_type": "code",
      "source": [
        "########################################### check  if CUDA available #################################################\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "########################################### MLP definition on GPU #################################################\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(1*28*28, 1024, device = device),   # device gets passed to every layer in the Sequential\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048, device = device), # device gets passed to every layer in the Sequential\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256, device = device),  # device gets passed to every layer in the Sequential\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 10, device = device)    # device gets passed to every layer in the Sequential\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "########################################### TRAINING ON GPU #################################################\n",
        "net = MLP(device = device)       #we build MLP on a device\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(8):\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs.to(device))  # we move batch data to computational_device\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels.to(device), reduction = \"mean\") # we move batch labels to computational_device\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "zL-ANzGeHPmq",
        "outputId": "7aab9b54-2df2-4cea-aef7-86d1411ea9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zL-ANzGeHPmq",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.305189847946167\n",
            "epoch: 0 batch: 1 current batch loss: 2.079038381576538\n",
            "epoch: 0 batch: 2 current batch loss: 1.646899700164795\n",
            "epoch: 0 batch: 3 current batch loss: 1.174643635749817\n",
            "epoch: 0 batch: 4 current batch loss: 0.9872110486030579\n",
            "epoch: 0 batch: 5 current batch loss: 0.9014471173286438\n",
            "epoch: 0 batch: 6 current batch loss: 0.9230421185493469\n",
            "epoch: 0 batch: 7 current batch loss: 0.6982876062393188\n",
            "epoch: 0 batch: 8 current batch loss: 0.6534783840179443\n",
            "epoch: 0 batch: 9 current batch loss: 0.6471075415611267\n",
            "epoch: 0 batch: 10 current batch loss: 0.5857945084571838\n",
            "epoch: 0 batch: 11 current batch loss: 0.5136983394622803\n",
            "epoch: 0 batch: 12 current batch loss: 0.46963220834732056\n",
            "epoch: 0 batch: 13 current batch loss: 0.4716751277446747\n",
            "epoch: 0 batch: 14 current batch loss: 0.43089208006858826\n",
            "epoch: 0 batch: 15 current batch loss: 0.40062862634658813\n",
            "epoch: 0 batch: 16 current batch loss: 0.4213091731071472\n",
            "epoch: 0 batch: 17 current batch loss: 0.37193357944488525\n",
            "epoch: 0 batch: 18 current batch loss: 0.33242201805114746\n",
            "epoch: 0 batch: 19 current batch loss: 0.35140460729599\n",
            "epoch: 0 batch: 20 current batch loss: 0.34359607100486755\n",
            "epoch: 0 batch: 21 current batch loss: 0.3211607038974762\n",
            "epoch: 0 batch: 22 current batch loss: 0.288806676864624\n",
            "epoch: 0 batch: 23 current batch loss: 0.2832176685333252\n",
            "epoch: 0 batch: 24 current batch loss: 0.2585031986236572\n",
            "epoch: 0 batch: 25 current batch loss: 0.28375667333602905\n",
            "epoch: 0 batch: 26 current batch loss: 0.25645285844802856\n",
            "epoch: 0 batch: 27 current batch loss: 0.2370954006910324\n",
            "epoch: 0 batch: 28 current batch loss: 0.23489485681056976\n",
            "epoch: 0 batch: 29 current batch loss: 0.25271981954574585\n",
            "epoch: 1 batch: 0 current batch loss: 0.1986747831106186\n",
            "epoch: 1 batch: 1 current batch loss: 0.20948079228401184\n",
            "epoch: 1 batch: 2 current batch loss: 0.23522832989692688\n",
            "epoch: 1 batch: 3 current batch loss: 0.19513455033302307\n",
            "epoch: 1 batch: 4 current batch loss: 0.2081959992647171\n",
            "epoch: 1 batch: 5 current batch loss: 0.17915132641792297\n",
            "epoch: 1 batch: 6 current batch loss: 0.18365801870822906\n",
            "epoch: 1 batch: 7 current batch loss: 0.16020432114601135\n",
            "epoch: 1 batch: 8 current batch loss: 0.1824156492948532\n",
            "epoch: 1 batch: 9 current batch loss: 0.1686757504940033\n",
            "epoch: 1 batch: 10 current batch loss: 0.17284341156482697\n",
            "epoch: 1 batch: 11 current batch loss: 0.1725136637687683\n",
            "epoch: 1 batch: 12 current batch loss: 0.14985865354537964\n",
            "epoch: 1 batch: 13 current batch loss: 0.1608169674873352\n",
            "epoch: 1 batch: 14 current batch loss: 0.16367922723293304\n",
            "epoch: 1 batch: 15 current batch loss: 0.1735425889492035\n",
            "epoch: 1 batch: 16 current batch loss: 0.14820711314678192\n",
            "epoch: 1 batch: 17 current batch loss: 0.14486683905124664\n",
            "epoch: 1 batch: 18 current batch loss: 0.13431447744369507\n",
            "epoch: 1 batch: 19 current batch loss: 0.1367758810520172\n",
            "epoch: 1 batch: 20 current batch loss: 0.167949378490448\n",
            "epoch: 1 batch: 21 current batch loss: 0.16421645879745483\n",
            "epoch: 1 batch: 22 current batch loss: 0.12829358875751495\n",
            "epoch: 1 batch: 23 current batch loss: 0.15244129300117493\n",
            "epoch: 1 batch: 24 current batch loss: 0.14623470604419708\n",
            "epoch: 1 batch: 25 current batch loss: 0.13106906414031982\n",
            "epoch: 1 batch: 26 current batch loss: 0.15396754443645477\n",
            "epoch: 1 batch: 27 current batch loss: 0.12503626942634583\n",
            "epoch: 1 batch: 28 current batch loss: 0.13188664615154266\n",
            "epoch: 1 batch: 29 current batch loss: 0.1580539047718048\n",
            "epoch: 2 batch: 0 current batch loss: 0.10636454820632935\n",
            "epoch: 2 batch: 1 current batch loss: 0.11124034970998764\n",
            "epoch: 2 batch: 2 current batch loss: 0.10233674943447113\n",
            "epoch: 2 batch: 3 current batch loss: 0.10959423333406448\n",
            "epoch: 2 batch: 4 current batch loss: 0.10029072314500809\n",
            "epoch: 2 batch: 5 current batch loss: 0.09873037040233612\n",
            "epoch: 2 batch: 6 current batch loss: 0.11043044924736023\n",
            "epoch: 2 batch: 7 current batch loss: 0.10571632534265518\n",
            "epoch: 2 batch: 8 current batch loss: 0.11127609759569168\n",
            "epoch: 2 batch: 9 current batch loss: 0.09391304850578308\n",
            "epoch: 2 batch: 10 current batch loss: 0.11002357304096222\n",
            "epoch: 2 batch: 11 current batch loss: 0.07458925992250443\n",
            "epoch: 2 batch: 12 current batch loss: 0.11983221769332886\n",
            "epoch: 2 batch: 13 current batch loss: 0.09898818284273148\n",
            "epoch: 2 batch: 14 current batch loss: 0.10961219668388367\n",
            "epoch: 2 batch: 15 current batch loss: 0.09732313454151154\n",
            "epoch: 2 batch: 16 current batch loss: 0.08950864523649216\n",
            "epoch: 2 batch: 17 current batch loss: 0.09326847642660141\n",
            "epoch: 2 batch: 18 current batch loss: 0.08865340054035187\n",
            "epoch: 2 batch: 19 current batch loss: 0.09009042382240295\n",
            "epoch: 2 batch: 20 current batch loss: 0.09855818748474121\n",
            "epoch: 2 batch: 21 current batch loss: 0.0981164500117302\n",
            "epoch: 2 batch: 22 current batch loss: 0.08452725410461426\n",
            "epoch: 2 batch: 23 current batch loss: 0.07806877791881561\n",
            "epoch: 2 batch: 24 current batch loss: 0.0951884314417839\n",
            "epoch: 2 batch: 25 current batch loss: 0.07927677780389786\n",
            "epoch: 2 batch: 26 current batch loss: 0.08331052958965302\n",
            "epoch: 2 batch: 27 current batch loss: 0.09249147027730942\n",
            "epoch: 2 batch: 28 current batch loss: 0.10175421833992004\n",
            "epoch: 2 batch: 29 current batch loss: 0.0718921646475792\n",
            "epoch: 3 batch: 0 current batch loss: 0.06104501709342003\n",
            "epoch: 3 batch: 1 current batch loss: 0.06735862791538239\n",
            "epoch: 3 batch: 2 current batch loss: 0.06801928579807281\n",
            "epoch: 3 batch: 3 current batch loss: 0.0723036453127861\n",
            "epoch: 3 batch: 4 current batch loss: 0.055987972766160965\n",
            "epoch: 3 batch: 5 current batch loss: 0.07521605491638184\n",
            "epoch: 3 batch: 6 current batch loss: 0.064505934715271\n",
            "epoch: 3 batch: 7 current batch loss: 0.06435451656579971\n",
            "epoch: 3 batch: 8 current batch loss: 0.07399610430002213\n",
            "epoch: 3 batch: 9 current batch loss: 0.07496900111436844\n",
            "epoch: 3 batch: 10 current batch loss: 0.06933414936065674\n",
            "epoch: 3 batch: 11 current batch loss: 0.05292246490716934\n",
            "epoch: 3 batch: 12 current batch loss: 0.05620352178812027\n",
            "epoch: 3 batch: 13 current batch loss: 0.07186218351125717\n",
            "epoch: 3 batch: 14 current batch loss: 0.062382861971855164\n",
            "epoch: 3 batch: 15 current batch loss: 0.054702408611774445\n",
            "epoch: 3 batch: 16 current batch loss: 0.050815124064683914\n",
            "epoch: 3 batch: 17 current batch loss: 0.05649322271347046\n",
            "epoch: 3 batch: 18 current batch loss: 0.053882814943790436\n",
            "epoch: 3 batch: 19 current batch loss: 0.07270529866218567\n",
            "epoch: 3 batch: 20 current batch loss: 0.07285388559103012\n",
            "epoch: 3 batch: 21 current batch loss: 0.059511855244636536\n",
            "epoch: 3 batch: 22 current batch loss: 0.059023089706897736\n",
            "epoch: 3 batch: 23 current batch loss: 0.061523132026195526\n",
            "epoch: 3 batch: 24 current batch loss: 0.05906147137284279\n",
            "epoch: 3 batch: 25 current batch loss: 0.0688677579164505\n",
            "epoch: 3 batch: 26 current batch loss: 0.054893508553504944\n",
            "epoch: 3 batch: 27 current batch loss: 0.06476952880620956\n",
            "epoch: 3 batch: 28 current batch loss: 0.06620830297470093\n",
            "epoch: 3 batch: 29 current batch loss: 0.07360158115625381\n",
            "epoch: 4 batch: 0 current batch loss: 0.05878924950957298\n",
            "epoch: 4 batch: 1 current batch loss: 0.04465462267398834\n",
            "epoch: 4 batch: 2 current batch loss: 0.04347743093967438\n",
            "epoch: 4 batch: 3 current batch loss: 0.045259322971105576\n",
            "epoch: 4 batch: 4 current batch loss: 0.05912884324789047\n",
            "epoch: 4 batch: 5 current batch loss: 0.04871969297528267\n",
            "epoch: 4 batch: 6 current batch loss: 0.04201440513134003\n",
            "epoch: 4 batch: 7 current batch loss: 0.0478844940662384\n",
            "epoch: 4 batch: 8 current batch loss: 0.05566449463367462\n",
            "epoch: 4 batch: 9 current batch loss: 0.035479314625263214\n",
            "epoch: 4 batch: 10 current batch loss: 0.04439534619450569\n",
            "epoch: 4 batch: 11 current batch loss: 0.05233137309551239\n",
            "epoch: 4 batch: 12 current batch loss: 0.04496924579143524\n",
            "epoch: 4 batch: 13 current batch loss: 0.035709358751773834\n",
            "epoch: 4 batch: 14 current batch loss: 0.03517549857497215\n",
            "epoch: 4 batch: 15 current batch loss: 0.05173882842063904\n",
            "epoch: 4 batch: 16 current batch loss: 0.03069104626774788\n",
            "epoch: 4 batch: 17 current batch loss: 0.04714692756533623\n",
            "epoch: 4 batch: 18 current batch loss: 0.05247650668025017\n",
            "epoch: 4 batch: 19 current batch loss: 0.044149212539196014\n",
            "epoch: 4 batch: 20 current batch loss: 0.047324031591415405\n",
            "epoch: 4 batch: 21 current batch loss: 0.058815959841012955\n",
            "epoch: 4 batch: 22 current batch loss: 0.04361468181014061\n",
            "epoch: 4 batch: 23 current batch loss: 0.05034288018941879\n",
            "epoch: 4 batch: 24 current batch loss: 0.05660877004265785\n",
            "epoch: 4 batch: 25 current batch loss: 0.04432137310504913\n",
            "epoch: 4 batch: 26 current batch loss: 0.03957163915038109\n",
            "epoch: 4 batch: 27 current batch loss: 0.0441504642367363\n",
            "epoch: 4 batch: 28 current batch loss: 0.04866250604391098\n",
            "epoch: 4 batch: 29 current batch loss: 0.04593838378787041\n",
            "epoch: 5 batch: 0 current batch loss: 0.02061617374420166\n",
            "epoch: 5 batch: 1 current batch loss: 0.025961915031075478\n",
            "epoch: 5 batch: 2 current batch loss: 0.0349883958697319\n",
            "epoch: 5 batch: 3 current batch loss: 0.046569954603910446\n",
            "epoch: 5 batch: 4 current batch loss: 0.02753026783466339\n",
            "epoch: 5 batch: 5 current batch loss: 0.03426129370927811\n",
            "epoch: 5 batch: 6 current batch loss: 0.03274037316441536\n",
            "epoch: 5 batch: 7 current batch loss: 0.03346581012010574\n",
            "epoch: 5 batch: 8 current batch loss: 0.03184773027896881\n",
            "epoch: 5 batch: 9 current batch loss: 0.026035450398921967\n",
            "epoch: 5 batch: 10 current batch loss: 0.029846735298633575\n",
            "epoch: 5 batch: 11 current batch loss: 0.028581783175468445\n",
            "epoch: 5 batch: 12 current batch loss: 0.02416941151022911\n",
            "epoch: 5 batch: 13 current batch loss: 0.027379896491765976\n",
            "epoch: 5 batch: 14 current batch loss: 0.03457130491733551\n",
            "epoch: 5 batch: 15 current batch loss: 0.029821345582604408\n",
            "epoch: 5 batch: 16 current batch loss: 0.02436235547065735\n",
            "epoch: 5 batch: 17 current batch loss: 0.03180375322699547\n",
            "epoch: 5 batch: 18 current batch loss: 0.02478119172155857\n",
            "epoch: 5 batch: 19 current batch loss: 0.030159421265125275\n",
            "epoch: 5 batch: 20 current batch loss: 0.023755759000778198\n",
            "epoch: 5 batch: 21 current batch loss: 0.026819180697202682\n",
            "epoch: 5 batch: 22 current batch loss: 0.03402586281299591\n",
            "epoch: 5 batch: 23 current batch loss: 0.02662108652293682\n",
            "epoch: 5 batch: 24 current batch loss: 0.02873145416378975\n",
            "epoch: 5 batch: 25 current batch loss: 0.027128487825393677\n",
            "epoch: 5 batch: 26 current batch loss: 0.03645278885960579\n",
            "epoch: 5 batch: 27 current batch loss: 0.035989150404930115\n",
            "epoch: 5 batch: 28 current batch loss: 0.03475501760840416\n",
            "epoch: 5 batch: 29 current batch loss: 0.0461866557598114\n",
            "epoch: 6 batch: 0 current batch loss: 0.017201922833919525\n",
            "epoch: 6 batch: 1 current batch loss: 0.02333185262978077\n",
            "epoch: 6 batch: 2 current batch loss: 0.02263827621936798\n",
            "epoch: 6 batch: 3 current batch loss: 0.02108791097998619\n",
            "epoch: 6 batch: 4 current batch loss: 0.020125724375247955\n",
            "epoch: 6 batch: 5 current batch loss: 0.030178038403391838\n",
            "epoch: 6 batch: 6 current batch loss: 0.027724528685212135\n",
            "epoch: 6 batch: 7 current batch loss: 0.024426640942692757\n",
            "epoch: 6 batch: 8 current batch loss: 0.015380065888166428\n",
            "epoch: 6 batch: 9 current batch loss: 0.018419092521071434\n",
            "epoch: 6 batch: 10 current batch loss: 0.028613364323973656\n",
            "epoch: 6 batch: 11 current batch loss: 0.022725438699126244\n",
            "epoch: 6 batch: 12 current batch loss: 0.029342297464609146\n",
            "epoch: 6 batch: 13 current batch loss: 0.021343642845749855\n",
            "epoch: 6 batch: 14 current batch loss: 0.020068421959877014\n",
            "epoch: 6 batch: 15 current batch loss: 0.025869254022836685\n",
            "epoch: 6 batch: 16 current batch loss: 0.015883661806583405\n",
            "epoch: 6 batch: 17 current batch loss: 0.01781514845788479\n",
            "epoch: 6 batch: 18 current batch loss: 0.01980583555996418\n",
            "epoch: 6 batch: 19 current batch loss: 0.02593749389052391\n",
            "epoch: 6 batch: 20 current batch loss: 0.03156096115708351\n",
            "epoch: 6 batch: 21 current batch loss: 0.01841096580028534\n",
            "epoch: 6 batch: 22 current batch loss: 0.020602578297257423\n",
            "epoch: 6 batch: 23 current batch loss: 0.02953256666660309\n",
            "epoch: 6 batch: 24 current batch loss: 0.02171415276825428\n",
            "epoch: 6 batch: 25 current batch loss: 0.018230943009257317\n",
            "epoch: 6 batch: 26 current batch loss: 0.02320737950503826\n",
            "epoch: 6 batch: 27 current batch loss: 0.020953956991434097\n",
            "epoch: 6 batch: 28 current batch loss: 0.01928945817053318\n",
            "epoch: 6 batch: 29 current batch loss: 0.013566377572715282\n",
            "epoch: 7 batch: 0 current batch loss: 0.014505408704280853\n",
            "epoch: 7 batch: 1 current batch loss: 0.02228423021733761\n",
            "epoch: 7 batch: 2 current batch loss: 0.013412287458777428\n",
            "epoch: 7 batch: 3 current batch loss: 0.012438471429049969\n",
            "epoch: 7 batch: 4 current batch loss: 0.01645197719335556\n",
            "epoch: 7 batch: 5 current batch loss: 0.012426117435097694\n",
            "epoch: 7 batch: 6 current batch loss: 0.019701723009347916\n",
            "epoch: 7 batch: 7 current batch loss: 0.011036359705030918\n",
            "epoch: 7 batch: 8 current batch loss: 0.017181843519210815\n",
            "epoch: 7 batch: 9 current batch loss: 0.019939390942454338\n",
            "epoch: 7 batch: 10 current batch loss: 0.014307758770883083\n",
            "epoch: 7 batch: 11 current batch loss: 0.01215211022645235\n",
            "epoch: 7 batch: 12 current batch loss: 0.016982121393084526\n",
            "epoch: 7 batch: 13 current batch loss: 0.021674668416380882\n",
            "epoch: 7 batch: 14 current batch loss: 0.03131166473031044\n",
            "epoch: 7 batch: 15 current batch loss: 0.020575640723109245\n",
            "epoch: 7 batch: 16 current batch loss: 0.022876087576150894\n",
            "epoch: 7 batch: 17 current batch loss: 0.024922994896769524\n",
            "epoch: 7 batch: 18 current batch loss: 0.017889976501464844\n",
            "epoch: 7 batch: 19 current batch loss: 0.018328819423913956\n",
            "epoch: 7 batch: 20 current batch loss: 0.017796970903873444\n",
            "epoch: 7 batch: 21 current batch loss: 0.016889410093426704\n",
            "epoch: 7 batch: 22 current batch loss: 0.014943151734769344\n",
            "epoch: 7 batch: 23 current batch loss: 0.02407495491206646\n",
            "epoch: 7 batch: 24 current batch loss: 0.017742954194545746\n",
            "epoch: 7 batch: 25 current batch loss: 0.021202750504016876\n",
            "epoch: 7 batch: 26 current batch loss: 0.013704657554626465\n",
            "epoch: 7 batch: 27 current batch loss: 0.01230423990637064\n",
            "epoch: 7 batch: 28 current batch loss: 0.012537217698991299\n",
            "epoch: 7 batch: 29 current batch loss: 0.02454196847975254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ TESTING ON GPU #####################################################\n",
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for batch, data in enumerate(testloader):\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  # moving a batch to GPU\n",
        "        classification = torch.argmax(prediction)\n",
        "\n",
        "        if classification.item() == label.item():     # the comparison is on item() which is not a tensor, i.e. there is no need to move labels to GPU\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ],
      "metadata": {
        "id": "5Bb3X3kFHex8",
        "outputId": "1dde5ee0-c6d1-425c-891a-0d2425e41bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5Bb3X3kFHex8",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.982\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}