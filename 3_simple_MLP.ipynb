{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers separated by nonlinear components (also called *activation functions*). A nonlinear component in-between the linear layers is essential: without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. Also, please note that it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. You can read more about this dataset here: https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "train_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "### Your task #1\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "### Your task #2\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1306604762738429, 0.30810780385646264)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=False)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([5, 0, 4,  ..., 1, 4, 1])\n",
      "1 -th batch labels : tensor([7, 5, 4,  ..., 9, 3, 9])\n",
      "2 -th batch labels : tensor([2, 4, 9,  ..., 7, 1, 2])\n",
      "3 -th batch labels : tensor([2, 9, 0,  ..., 1, 5, 6])\n",
      "4 -th batch labels : tensor([3, 0, 1,  ..., 0, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "### Your taks #3\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i==0:\n",
    "            print(i, \"-th batch inputs :\", batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.371558666229248\n",
      "epoch: 0 batch: 1 current batch loss: 2.38930344581604\n",
      "epoch: 0 batch: 2 current batch loss: 2.368551015853882\n",
      "epoch: 0 batch: 3 current batch loss: 2.3257222175598145\n",
      "epoch: 0 batch: 4 current batch loss: 2.2956409454345703\n",
      "epoch: 0 batch: 5 current batch loss: 2.286395311355591\n",
      "epoch: 0 batch: 6 current batch loss: 2.280097246170044\n",
      "epoch: 0 batch: 7 current batch loss: 2.2642595767974854\n",
      "epoch: 0 batch: 8 current batch loss: 2.2483108043670654\n",
      "epoch: 0 batch: 9 current batch loss: 2.204660415649414\n",
      "epoch: 0 batch: 10 current batch loss: 2.1567130088806152\n",
      "epoch: 0 batch: 11 current batch loss: 2.109605550765991\n",
      "epoch: 0 batch: 12 current batch loss: 2.0402870178222656\n",
      "epoch: 0 batch: 13 current batch loss: 1.9680569171905518\n",
      "epoch: 0 batch: 14 current batch loss: 1.9065264463424683\n",
      "epoch: 0 batch: 15 current batch loss: 1.8569889068603516\n",
      "epoch: 0 batch: 16 current batch loss: 1.727373719215393\n",
      "epoch: 0 batch: 17 current batch loss: 1.6729172468185425\n",
      "epoch: 0 batch: 18 current batch loss: 1.5925612449645996\n",
      "epoch: 0 batch: 19 current batch loss: 1.4935086965560913\n",
      "epoch: 0 batch: 20 current batch loss: 1.44047212600708\n",
      "epoch: 0 batch: 21 current batch loss: 1.370355486869812\n",
      "epoch: 0 batch: 22 current batch loss: 1.2963086366653442\n",
      "epoch: 0 batch: 23 current batch loss: 1.2557504177093506\n",
      "epoch: 0 batch: 24 current batch loss: 1.217349886894226\n",
      "epoch: 0 batch: 25 current batch loss: 1.1522727012634277\n",
      "epoch: 0 batch: 26 current batch loss: 1.0830332040786743\n",
      "epoch: 0 batch: 27 current batch loss: 1.0441299676895142\n",
      "epoch: 0 batch: 28 current batch loss: 0.9569914937019348\n",
      "epoch: 0 batch: 29 current batch loss: 0.9111528992652893\n",
      "epoch: 1 batch: 0 current batch loss: 0.955593466758728\n",
      "epoch: 1 batch: 1 current batch loss: 0.8812695145606995\n",
      "epoch: 1 batch: 2 current batch loss: 0.8321528434753418\n",
      "epoch: 1 batch: 3 current batch loss: 0.8432091474533081\n",
      "epoch: 1 batch: 4 current batch loss: 0.8285717368125916\n",
      "epoch: 1 batch: 5 current batch loss: 0.7885882258415222\n",
      "epoch: 1 batch: 6 current batch loss: 0.7931857705116272\n",
      "epoch: 1 batch: 7 current batch loss: 0.765820324420929\n",
      "epoch: 1 batch: 8 current batch loss: 0.7473985552787781\n",
      "epoch: 1 batch: 9 current batch loss: 0.6575568318367004\n",
      "epoch: 1 batch: 10 current batch loss: 0.6618536114692688\n",
      "epoch: 1 batch: 11 current batch loss: 0.6334442496299744\n",
      "epoch: 1 batch: 12 current batch loss: 0.6359957456588745\n",
      "epoch: 1 batch: 13 current batch loss: 0.6256822943687439\n",
      "epoch: 1 batch: 14 current batch loss: 0.6153630614280701\n",
      "epoch: 1 batch: 15 current batch loss: 0.6378753781318665\n",
      "epoch: 1 batch: 16 current batch loss: 0.5392200350761414\n",
      "epoch: 1 batch: 17 current batch loss: 0.5118466019630432\n",
      "epoch: 1 batch: 18 current batch loss: 0.5527002215385437\n",
      "epoch: 1 batch: 19 current batch loss: 0.5121027231216431\n",
      "epoch: 1 batch: 20 current batch loss: 0.5439698696136475\n",
      "epoch: 1 batch: 21 current batch loss: 0.4789811372756958\n",
      "epoch: 1 batch: 22 current batch loss: 0.48625069856643677\n",
      "epoch: 1 batch: 23 current batch loss: 0.4735451340675354\n",
      "epoch: 1 batch: 24 current batch loss: 0.500584065914154\n",
      "epoch: 1 batch: 25 current batch loss: 0.46594947576522827\n",
      "epoch: 1 batch: 26 current batch loss: 0.40157902240753174\n",
      "epoch: 1 batch: 27 current batch loss: 0.3799860179424286\n",
      "epoch: 1 batch: 28 current batch loss: 0.3343221843242645\n",
      "epoch: 1 batch: 29 current batch loss: 0.3329642713069916\n",
      "epoch: 2 batch: 0 current batch loss: 0.4070494771003723\n",
      "epoch: 2 batch: 1 current batch loss: 0.3461993932723999\n",
      "epoch: 2 batch: 2 current batch loss: 0.35749518871307373\n",
      "epoch: 2 batch: 3 current batch loss: 0.3840722441673279\n",
      "epoch: 2 batch: 4 current batch loss: 0.4137849509716034\n",
      "epoch: 2 batch: 5 current batch loss: 0.35940927267074585\n",
      "epoch: 2 batch: 6 current batch loss: 0.41817033290863037\n",
      "epoch: 2 batch: 7 current batch loss: 0.3844988942146301\n",
      "epoch: 2 batch: 8 current batch loss: 0.36389413475990295\n",
      "epoch: 2 batch: 9 current batch loss: 0.3153822124004364\n",
      "epoch: 2 batch: 10 current batch loss: 0.3358263075351715\n",
      "epoch: 2 batch: 11 current batch loss: 0.33447548747062683\n",
      "epoch: 2 batch: 12 current batch loss: 0.3357347548007965\n",
      "epoch: 2 batch: 13 current batch loss: 0.3434198200702667\n",
      "epoch: 2 batch: 14 current batch loss: 0.343479186296463\n",
      "epoch: 2 batch: 15 current batch loss: 0.3797144889831543\n",
      "epoch: 2 batch: 16 current batch loss: 0.30271410942077637\n",
      "epoch: 2 batch: 17 current batch loss: 0.28508350253105164\n",
      "epoch: 2 batch: 18 current batch loss: 0.3594053089618683\n",
      "epoch: 2 batch: 19 current batch loss: 0.3124898672103882\n",
      "epoch: 2 batch: 20 current batch loss: 0.34998998045921326\n",
      "epoch: 2 batch: 21 current batch loss: 0.28317564725875854\n",
      "epoch: 2 batch: 22 current batch loss: 0.31266671419143677\n",
      "epoch: 2 batch: 23 current batch loss: 0.30723458528518677\n",
      "epoch: 2 batch: 24 current batch loss: 0.3514966070652008\n",
      "epoch: 2 batch: 25 current batch loss: 0.3063179552555084\n",
      "epoch: 2 batch: 26 current batch loss: 0.26699820160865784\n",
      "epoch: 2 batch: 27 current batch loss: 0.24330636858940125\n",
      "epoch: 2 batch: 28 current batch loss: 0.20723815262317657\n",
      "epoch: 2 batch: 29 current batch loss: 0.20503921806812286\n",
      "epoch: 3 batch: 0 current batch loss: 0.28608277440071106\n",
      "epoch: 3 batch: 1 current batch loss: 0.22490182518959045\n",
      "epoch: 3 batch: 2 current batch loss: 0.25038325786590576\n",
      "epoch: 3 batch: 3 current batch loss: 0.25789347290992737\n",
      "epoch: 3 batch: 4 current batch loss: 0.3104042410850525\n",
      "epoch: 3 batch: 5 current batch loss: 0.24963560700416565\n",
      "epoch: 3 batch: 6 current batch loss: 0.30872446298599243\n",
      "epoch: 3 batch: 7 current batch loss: 0.2847858965396881\n",
      "epoch: 3 batch: 8 current batch loss: 0.2640845477581024\n",
      "epoch: 3 batch: 9 current batch loss: 0.23287808895111084\n",
      "epoch: 3 batch: 10 current batch loss: 0.2549234628677368\n",
      "epoch: 3 batch: 11 current batch loss: 0.24318483471870422\n",
      "epoch: 3 batch: 12 current batch loss: 0.25524818897247314\n",
      "epoch: 3 batch: 13 current batch loss: 0.2663644850254059\n",
      "epoch: 3 batch: 14 current batch loss: 0.2472347766160965\n",
      "epoch: 3 batch: 15 current batch loss: 0.2846038341522217\n",
      "epoch: 3 batch: 16 current batch loss: 0.22817076742649078\n",
      "epoch: 3 batch: 17 current batch loss: 0.212947279214859\n",
      "epoch: 3 batch: 18 current batch loss: 0.2706036865711212\n",
      "epoch: 3 batch: 19 current batch loss: 0.22849541902542114\n",
      "epoch: 3 batch: 20 current batch loss: 0.2732691168785095\n",
      "epoch: 3 batch: 21 current batch loss: 0.21111151576042175\n",
      "epoch: 3 batch: 22 current batch loss: 0.23448647558689117\n",
      "epoch: 3 batch: 23 current batch loss: 0.23210565745830536\n",
      "epoch: 3 batch: 24 current batch loss: 0.26934105157852173\n",
      "epoch: 3 batch: 25 current batch loss: 0.23904719948768616\n",
      "epoch: 3 batch: 26 current batch loss: 0.20337440073490143\n",
      "epoch: 3 batch: 27 current batch loss: 0.17974479496479034\n",
      "epoch: 3 batch: 28 current batch loss: 0.15051566064357758\n",
      "epoch: 3 batch: 29 current batch loss: 0.15284861624240875\n",
      "epoch: 4 batch: 0 current batch loss: 0.22390514612197876\n",
      "epoch: 4 batch: 1 current batch loss: 0.1645388901233673\n",
      "epoch: 4 batch: 2 current batch loss: 0.1962037980556488\n",
      "epoch: 4 batch: 3 current batch loss: 0.19649182260036469\n",
      "epoch: 4 batch: 4 current batch loss: 0.2425992488861084\n",
      "epoch: 4 batch: 5 current batch loss: 0.18604350090026855\n",
      "epoch: 4 batch: 6 current batch loss: 0.24463129043579102\n",
      "epoch: 4 batch: 7 current batch loss: 0.2276017665863037\n",
      "epoch: 4 batch: 8 current batch loss: 0.20484887063503265\n",
      "epoch: 4 batch: 9 current batch loss: 0.1802934855222702\n",
      "epoch: 4 batch: 10 current batch loss: 0.20734803378582\n",
      "epoch: 4 batch: 11 current batch loss: 0.19072669744491577\n",
      "epoch: 4 batch: 12 current batch loss: 0.20966140925884247\n",
      "epoch: 4 batch: 13 current batch loss: 0.2129111886024475\n",
      "epoch: 4 batch: 14 current batch loss: 0.18896280229091644\n",
      "epoch: 4 batch: 15 current batch loss: 0.2235439568758011\n",
      "epoch: 4 batch: 16 current batch loss: 0.18064159154891968\n",
      "epoch: 4 batch: 17 current batch loss: 0.16608788073062897\n",
      "epoch: 4 batch: 18 current batch loss: 0.21465341746807098\n",
      "epoch: 4 batch: 19 current batch loss: 0.17710164189338684\n",
      "epoch: 4 batch: 20 current batch loss: 0.22061684727668762\n",
      "epoch: 4 batch: 21 current batch loss: 0.16565123200416565\n",
      "epoch: 4 batch: 22 current batch loss: 0.18532542884349823\n",
      "epoch: 4 batch: 23 current batch loss: 0.1877860724925995\n",
      "epoch: 4 batch: 24 current batch loss: 0.21233320236206055\n",
      "epoch: 4 batch: 25 current batch loss: 0.19215653836727142\n",
      "epoch: 4 batch: 26 current batch loss: 0.1571492701768875\n",
      "epoch: 4 batch: 27 current batch loss: 0.13918638229370117\n",
      "epoch: 4 batch: 28 current batch loss: 0.11414063721895218\n",
      "epoch: 4 batch: 29 current batch loss: 0.11985871940851212\n",
      "epoch: 5 batch: 0 current batch loss: 0.18150173127651215\n",
      "epoch: 5 batch: 1 current batch loss: 0.12213516235351562\n",
      "epoch: 5 batch: 2 current batch loss: 0.1568119376897812\n",
      "epoch: 5 batch: 3 current batch loss: 0.1594667285680771\n",
      "epoch: 5 batch: 4 current batch loss: 0.1936701387166977\n",
      "epoch: 5 batch: 5 current batch loss: 0.1452605277299881\n",
      "epoch: 5 batch: 6 current batch loss: 0.19472862780094147\n",
      "epoch: 5 batch: 7 current batch loss: 0.18318021297454834\n",
      "epoch: 5 batch: 8 current batch loss: 0.16329196095466614\n",
      "epoch: 5 batch: 9 current batch loss: 0.14435404539108276\n",
      "epoch: 5 batch: 10 current batch loss: 0.17418310046195984\n",
      "epoch: 5 batch: 11 current batch loss: 0.15140558779239655\n",
      "epoch: 5 batch: 12 current batch loss: 0.1754038780927658\n",
      "epoch: 5 batch: 13 current batch loss: 0.17413674294948578\n",
      "epoch: 5 batch: 14 current batch loss: 0.1472148895263672\n",
      "epoch: 5 batch: 15 current batch loss: 0.17984436452388763\n",
      "epoch: 5 batch: 16 current batch loss: 0.14736510813236237\n",
      "epoch: 5 batch: 17 current batch loss: 0.13683797419071198\n",
      "epoch: 5 batch: 18 current batch loss: 0.1731075644493103\n",
      "epoch: 5 batch: 19 current batch loss: 0.14026649296283722\n",
      "epoch: 5 batch: 20 current batch loss: 0.18440531194210052\n",
      "epoch: 5 batch: 21 current batch loss: 0.13448821008205414\n",
      "epoch: 5 batch: 22 current batch loss: 0.15264852344989777\n",
      "epoch: 5 batch: 23 current batch loss: 0.1548650711774826\n",
      "epoch: 5 batch: 24 current batch loss: 0.16989204287528992\n",
      "epoch: 5 batch: 25 current batch loss: 0.15825065970420837\n",
      "epoch: 5 batch: 26 current batch loss: 0.12484405189752579\n",
      "epoch: 5 batch: 27 current batch loss: 0.11186852306127548\n",
      "epoch: 5 batch: 28 current batch loss: 0.0900031179189682\n",
      "epoch: 5 batch: 29 current batch loss: 0.10147091001272202\n",
      "epoch: 6 batch: 0 current batch loss: 0.14995397627353668\n",
      "epoch: 6 batch: 1 current batch loss: 0.09217053651809692\n",
      "epoch: 6 batch: 2 current batch loss: 0.12916824221611023\n",
      "epoch: 6 batch: 3 current batch loss: 0.1349010467529297\n",
      "epoch: 6 batch: 4 current batch loss: 0.1572204977273941\n",
      "epoch: 6 batch: 5 current batch loss: 0.11860686540603638\n",
      "epoch: 6 batch: 6 current batch loss: 0.15808144211769104\n",
      "epoch: 6 batch: 7 current batch loss: 0.15365250408649445\n",
      "epoch: 6 batch: 8 current batch loss: 0.13355077803134918\n",
      "epoch: 6 batch: 9 current batch loss: 0.12046609073877335\n",
      "epoch: 6 batch: 10 current batch loss: 0.1502416431903839\n",
      "epoch: 6 batch: 11 current batch loss: 0.12175098061561584\n",
      "epoch: 6 batch: 12 current batch loss: 0.14788247644901276\n",
      "epoch: 6 batch: 13 current batch loss: 0.1450064331293106\n",
      "epoch: 6 batch: 14 current batch loss: 0.11765505373477936\n",
      "epoch: 6 batch: 15 current batch loss: 0.1453171819448471\n",
      "epoch: 6 batch: 16 current batch loss: 0.12301551550626755\n",
      "epoch: 6 batch: 17 current batch loss: 0.1185094341635704\n",
      "epoch: 6 batch: 18 current batch loss: 0.14229607582092285\n",
      "epoch: 6 batch: 19 current batch loss: 0.11302843689918518\n",
      "epoch: 6 batch: 20 current batch loss: 0.15694597363471985\n",
      "epoch: 6 batch: 21 current batch loss: 0.11584476381540298\n",
      "epoch: 6 batch: 22 current batch loss: 0.1297982931137085\n",
      "epoch: 6 batch: 23 current batch loss: 0.12834876775741577\n",
      "epoch: 6 batch: 24 current batch loss: 0.13856610655784607\n",
      "epoch: 6 batch: 25 current batch loss: 0.1347866654396057\n",
      "epoch: 6 batch: 26 current batch loss: 0.10152275860309601\n",
      "epoch: 6 batch: 27 current batch loss: 0.09075386077165604\n",
      "epoch: 6 batch: 28 current batch loss: 0.07220523804426193\n",
      "epoch: 6 batch: 29 current batch loss: 0.08695640414953232\n",
      "epoch: 7 batch: 0 current batch loss: 0.12459400296211243\n",
      "epoch: 7 batch: 1 current batch loss: 0.07263568043708801\n",
      "epoch: 7 batch: 2 current batch loss: 0.10778088867664337\n",
      "epoch: 7 batch: 3 current batch loss: 0.11801254004240036\n",
      "epoch: 7 batch: 4 current batch loss: 0.12971699237823486\n",
      "epoch: 7 batch: 5 current batch loss: 0.09622035175561905\n",
      "epoch: 7 batch: 6 current batch loss: 0.1312665343284607\n",
      "epoch: 7 batch: 7 current batch loss: 0.1320960968732834\n",
      "epoch: 7 batch: 8 current batch loss: 0.11018907278776169\n",
      "epoch: 7 batch: 9 current batch loss: 0.10234347730875015\n",
      "epoch: 7 batch: 10 current batch loss: 0.13102169334888458\n",
      "epoch: 7 batch: 11 current batch loss: 0.09926792234182358\n",
      "epoch: 7 batch: 12 current batch loss: 0.124763622879982\n",
      "epoch: 7 batch: 13 current batch loss: 0.12048104405403137\n",
      "epoch: 7 batch: 14 current batch loss: 0.09667760133743286\n",
      "epoch: 7 batch: 15 current batch loss: 0.11996085941791534\n",
      "epoch: 7 batch: 16 current batch loss: 0.10272253304719925\n",
      "epoch: 7 batch: 17 current batch loss: 0.10390233993530273\n",
      "epoch: 7 batch: 18 current batch loss: 0.12284203618764877\n",
      "epoch: 7 batch: 19 current batch loss: 0.09376678615808487\n",
      "epoch: 7 batch: 20 current batch loss: 0.1322527527809143\n",
      "epoch: 7 batch: 21 current batch loss: 0.10163337737321854\n",
      "epoch: 7 batch: 22 current batch loss: 0.11268384754657745\n",
      "epoch: 7 batch: 23 current batch loss: 0.10857605189085007\n",
      "epoch: 7 batch: 24 current batch loss: 0.1137903481721878\n",
      "epoch: 7 batch: 25 current batch loss: 0.11744610220193863\n",
      "epoch: 7 batch: 26 current batch loss: 0.08645160496234894\n",
      "epoch: 7 batch: 27 current batch loss: 0.07552622258663177\n",
      "epoch: 7 batch: 28 current batch loss: 0.060081783682107925\n",
      "epoch: 7 batch: 29 current batch loss: 0.07352276891469955\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001\n",
    "\n",
    "net.train()\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\")\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "### Your task #4\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
   "metadata": {},
   "source": [
    "### Answers to task #4\n",
    "\n",
    "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, a second and subsequent epochs, i.e. a next runs through the whole data, consist of different batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
    "\n",
    "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3727755546569824 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.371875524520874 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.3651115894317627 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.326876401901245 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.302621841430664 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.2808690071105957 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.265956163406372 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.266913890838623 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.243178129196167 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.2029662132263184 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.1540374755859375 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.0983917713165283 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 2.0276150703430176 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.9486548900604248 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.8813809156417847 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.8138904571533203 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.6949456930160522 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.6254864931106567 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.5491828918457031 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.4610661268234253 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.4226617813110352 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.3504903316497803 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.287909984588623 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.248567819595337 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.1986970901489258 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.1497750282287598 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.0858880281448364 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 1.038711667060852 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.9684354066848755 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.9066781401634216 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.9714199900627136 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.9058747291564941 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.8693528771400452 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.8753619194030762 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.8622931838035583 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.8231513500213623 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.8423579335212708 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.8156593441963196 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.7927500009536743 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.7018259167671204 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.6941599249839783 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.6751303672790527 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.6672111749649048 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.6582246422767639 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.6488450765609741 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.6489290595054626 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.561439037322998 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.515914797782898 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.5768613219261169 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.5279535055160522 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.5589764714241028 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.49468669295310974 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.497154176235199 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.48003891110420227 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.5012285113334656 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.48014041781425476 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.42754173278808594 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.3804382085800171 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.33744287490844727 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.3456292450428009 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.41937416791915894 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.3596329689025879 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.3669949173927307 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.39282962679862976 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.41507625579833984 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.3756060004234314 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.4232986569404602 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.39839425683021545 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.37668082118034363 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.3298861086368561 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.3470918536186218 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.349622517824173 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.357256144285202 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.3627147376537323 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.3603838086128235 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.38436299562454224 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.31995898485183716 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.3069964647293091 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.37089046835899353 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.3212367594242096 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.37696558237075806 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.30341196060180664 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.3234546482563019 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.30871474742889404 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.36356931924819946 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.3174225091934204 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.28781211376190186 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.249227374792099 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.21743474900722504 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.22473064064979553 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.3051452934741974 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.2407602220773697 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.2637776732444763 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.2769142985343933 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.3165900409221649 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.2677965760231018 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.3198835849761963 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.30594784021377563 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.2830031216144562 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.2427067756652832 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.2712928056716919 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.26324397325515747 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.27276167273521423 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.28402355313301086 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.2703476846218109 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.3005009591579437 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.24260570108890533 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.23695290088653564 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.29091528058052063 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.24632544815540314 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.3085455000400543 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.23981785774230957 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.2568288743495941 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.24614617228507996 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.29475274682044983 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.2547132968902588 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.23011140525341034 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.19457662105560303 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.1692749410867691 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.17221656441688538 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.2506812512874603 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.1877516359090805 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.21698592603206635 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.22263170778751373 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.26131707429885864 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.21216970682144165 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.26565733551979065 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.25707730650901794 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.23093682527542114 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.19696733355522156 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.22744080424308777 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.213128000497818 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.227493017911911 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.2357688993215561 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.21879078447818756 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.24784070253372192 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.19435785710811615 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.19571930170059204 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.24392803013324738 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.1969471275806427 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.25682973861694336 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.1960526406764984 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.21650439500808716 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.20990826189517975 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.2430097907781601 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.21296831965446472 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.18556970357894897 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.15773983299732208 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.13909710943698883 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.14487992227077484 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.211895152926445 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.14574943482875824 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.1801638901233673 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.1899588406085968 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.22390513122081757 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.1702612340450287 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.2235056459903717 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.21468737721443176 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.1896549016237259 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.16693677008152008 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.20009543001651764 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.17827123403549194 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.1880122274160385 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.19820824265480042 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.18127372860908508 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.2137654721736908 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.16057096421718597 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.16258865594863892 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.2108422815799713 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.16793088614940643 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.2208838313817978 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.1605282872915268 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.18652451038360596 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.18275156617164612 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.20882409811019897 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.18406757712364197 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.15594644844532013 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.13250969350337982 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.1133752390742302 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.1275605708360672 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.18774455785751343 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.12036484479904175 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.155308336019516 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.16389672458171844 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.193751722574234 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.14342822134494781 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.19554001092910767 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.18585321307182312 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.15999823808670044 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.14127124845981598 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.17976589500904083 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.15732033550739288 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.16243745386600494 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.1734144538640976 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.15138599276542664 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.18498817086219788 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.14013473689556122 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.143962100148201 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.186939537525177 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.14443224668502808 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.19402442872524261 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.1384044736623764 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.16680707037448883 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.16266599297523499 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.18001039326190948 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.1608061045408249 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.1379338949918747 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.11812865734100342 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.09605926275253296 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.11492318660020828 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.16581645607948303 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.10052163153886795 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.13785433769226074 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.147301584482193 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.16946274042129517 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.12003196775913239 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.17076458036899567 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.1658235490322113 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.14144696295261383 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.1228637769818306 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.15818022191524506 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.1391020566225052 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.14411896467208862 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.15827496349811554 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.1315474510192871 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.1613471806049347 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.12516480684280396 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.12919025123119354 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.1674412339925766 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.1266539841890335 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.17328831553459167 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.12127614766359329 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.1502876579761505 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.14637665450572968 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.15954989194869995 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.1440979689359665 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.12317326664924622 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.10574717819690704 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.08373285084962845 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.10539843887090683 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\")\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "### Your task #5\n",
    "\n",
    "Well, it seems that we were able to get the learning rate to 0.06 without a scheduler. Can you bring it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "### Your task #6\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss to low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9659\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "#batches in test are of size 1\n",
    "for batch, data in enumerate(testloader):\n",
    "    datapoint, label = data\n",
    "    \n",
    "    prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "    classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "    \n",
    "    if classification.item() == label.item():\n",
    "        good += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9599\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "#batches in test are of size 1\n",
    "for batch, data in enumerate(testloader):\n",
    "    datapoint, label = data\n",
    "    \n",
    "    prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "    classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "    \n",
    "    if classification.item() == label.item():\n",
    "        good += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the layers in a neural network, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5c6ca-1055-4d9f-b6e0-3e4601f3919d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
